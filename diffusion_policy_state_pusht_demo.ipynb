{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ef0de47646ff4798bc109cb39d5498d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_052e24aba89648b9ad5bdc91a91a63f0",
              "IPY_MODEL_ee500ae747fe4dfe8dc87378999f78a1",
              "IPY_MODEL_f2d0ffbc78624aa481574f067f924381"
            ],
            "layout": "IPY_MODEL_728aa183d4e04e37bbf07a915d0ab8cb"
          }
        },
        "052e24aba89648b9ad5bdc91a91a63f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87369ac78d0d42568031a8de8a8dafe9",
            "placeholder": "​",
            "style": "IPY_MODEL_fbdb972d50c24877b985899576781997",
            "value": "Eval PushTStateEnv: "
          }
        },
        "ee500ae747fe4dfe8dc87378999f78a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a66baea8ff3e414183a9cd40601a4608",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0592bee52c1c49a0adfbc75b5d12ae16",
            "value": 200
          }
        },
        "f2d0ffbc78624aa481574f067f924381": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af26c7d9942c4750a6c475e7b06e28a4",
            "placeholder": "​",
            "style": "IPY_MODEL_9fd57981de8b4200890ae131f82b6eff",
            "value": " 201/? [00:25&lt;00:00,  8.76it/s, reward=0.91]"
          }
        },
        "728aa183d4e04e37bbf07a915d0ab8cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87369ac78d0d42568031a8de8a8dafe9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbdb972d50c24877b985899576781997": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a66baea8ff3e414183a9cd40601a4608": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0592bee52c1c49a0adfbc75b5d12ae16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af26c7d9942c4750a6c475e7b06e28a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fd57981de8b4200890ae131f82b6eff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/irxsnguyen/PushTdiffusionpolicy/blob/main/diffusion_policy_state_pusht_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Installing pip packages**\n",
        "#@markdown - Diffusion Model: [PyTorch](https://pytorch.org) & [HuggingFace diffusers](https://huggingface.co/docs/diffusers/index)\n",
        "#@markdown - Dataset Loading: [Zarr](https://zarr.readthedocs.io/en/stable/) & numcodecs\n",
        "#@markdown - Push-T Env: gym, pygame, pymunk & shapely\n",
        "!python --version\n",
        "!pip3 install torch==1.13.1 torchvision==0.14.1 diffusers==0.18.2 \\\n",
        "scikit-image==0.19.3 scikit-video==1.1.11 zarr==2.12.0 numcodecs==0.10.2 \\\n",
        "pygame==2.1.2 pymunk==6.2.1 gym==0.26.2 shapely==1.8.4 \\\n",
        "&> /dev/null # mute output\n",
        "!pip install --upgrade zarr torch torchvision diffusers scikit-image scikit-video \\\n",
        "numcodecs pygame pymunk gymnasium shapely"
      ],
      "metadata": {
        "id": "2QwO2gAgiJS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27e64f34-2c52-4951-9768-efea4311b07f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.11\n",
            "Collecting zarr\n",
            "  Downloading zarr-3.1.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (0.35.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (0.25.2)\n",
            "Collecting scikit-video\n",
            "  Downloading scikit_video-1.1.11-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting numcodecs\n",
            "  Downloading numcodecs-0.16.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.12/dist-packages (2.6.1)\n",
            "Collecting pymunk\n",
            "  Downloading pymunk-7.1.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.12/dist-packages (2.1.1)\n",
            "Collecting donfig>=0.8 (from zarr)\n",
            "  Downloading donfig-0.8.1.post1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from zarr) (2.0.2)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.12/dist-packages (from zarr) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9 in /usr/local/lib/python3.12/dist-packages (from zarr) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers) (8.7.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.34.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.6.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (1.16.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2025.8.28)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: cffi>=1.17.1 in /usr/local/lib/python3.12/dist-packages (from pymunk) (1.17.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.17.1->pymunk) (2.22)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from donfig>=0.8->zarr) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.34.0->diffusers) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.34.0->diffusers) (1.1.9)\n",
            "Collecting crc32c>=2.7 (from numcodecs[crc32c]>=0.14->zarr)\n",
            "  Downloading crc32c-2.7.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2025.8.3)\n",
            "Downloading zarr-3.1.2-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_video-1.1.11-py2.py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numcodecs-0.16.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymunk-7.1.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading donfig-0.8.1.post1-py3-none-any.whl (21 kB)\n",
            "Downloading crc32c-2.7.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numcodecs, donfig, crc32c, scikit-video, pymunk, zarr\n",
            "Successfully installed crc32c-2.7.1 donfig-0.8.1.post1 numcodecs-0.16.2 pymunk-7.1.0 scikit-video-1.1.11 zarr-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Imports**\n",
        "# diffusion policy import\n",
        "from typing import Tuple, Sequence, Dict, Union, Optional\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import collections\n",
        "import zarr\n",
        "import huggingface_hub\n",
        "import huggingface_hub.utils\n",
        "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
        "from diffusers.training_utils import EMAModel\n",
        "from diffusers.optimization import get_scheduler\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# env import\n",
        "import gymnasium as gym\n",
        "from gym import spaces\n",
        "import pygame\n",
        "import pymunk\n",
        "import pymunk.pygame_util\n",
        "from pymunk.space_debug_draw_options import SpaceDebugColor\n",
        "from pymunk.vec2d import Vec2d\n",
        "import shapely.geometry as sg\n",
        "import cv2\n",
        "import skimage.transform as st\n",
        "from skvideo.io import vwrite\n",
        "from IPython.display import Video\n",
        "import gdown\n",
        "import os"
      ],
      "metadata": {
        "id": "VrX4VTl5pYNq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e3b239a-62fd-4ba7-c4cb-97a34b31f6ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/skvideo/utils/stpyr.py:2: DeprecationWarning: scipy.misc is deprecated and will be removed in 2.0.0\n",
            "  import scipy.misc as sc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "# @markdown ### **Environment**\n",
        "# @markdown Defines a PyMunk-based Push-T environment `PushTEnv`.\n",
        "# @markdown\n",
        "# @markdown **Goal**: push the gray T-block into the green area.\n",
        "# @markdown\n",
        "# @markdown Adapted from [Implicit Behavior Cloning](https://implicitbc.github.io/)\n",
        "\n",
        "TYPE_AGENT = 1\n",
        "TYPE_BLOCK = 2\n",
        "TYPE_WALL  = 3\n",
        "\n",
        "positive_y_is_up: bool = False\n",
        "\"\"\"Make increasing values of y point upwards.\n",
        "\n",
        "When True::\n",
        "\n",
        "    y\n",
        "    ^\n",
        "    |      . (3, 3)\n",
        "    |\n",
        "    |   . (2, 2)\n",
        "    |\n",
        "    +------ > x\n",
        "\n",
        "When False::\n",
        "\n",
        "    +------ > x\n",
        "    |\n",
        "    |   . (2, 2)\n",
        "    |\n",
        "    |      . (3, 3)\n",
        "    v\n",
        "    y\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def to_pygame(p: Tuple[float, float], surface: pygame.Surface) -> Tuple[int, int]:\n",
        "    \"\"\"Convenience method to convert pymunk coordinates to pygame surface\n",
        "    local coordinates.\n",
        "\n",
        "    Note that in case positive_y_is_up is False, this function wont actually do\n",
        "    anything except converting the point to integers.\n",
        "    \"\"\"\n",
        "    if positive_y_is_up:\n",
        "        return round(p[0]), surface.get_height() - round(p[1])\n",
        "    else:\n",
        "        return round(p[0]), round(p[1])\n",
        "\n",
        "\n",
        "def light_color(color: SpaceDebugColor):\n",
        "    color = np.minimum(1.2 * np.float32([color.r, color.g, color.b, color.a]), np.float32([255]))\n",
        "    color = SpaceDebugColor(r=color[0], g=color[1], b=color[2], a=color[3])\n",
        "    return color\n",
        "\n",
        "class DrawOptions(pymunk.SpaceDebugDrawOptions):\n",
        "    def __init__(self, surface: pygame.Surface) -> None:\n",
        "        \"\"\"Draw a pymunk.Space on a pygame.Surface object.\n",
        "\n",
        "        Typical usage::\n",
        "\n",
        "        >>> import pymunk\n",
        "        >>> surface = pygame.Surface((10,10))\n",
        "        >>> space = pymunk.Space()\n",
        "        >>> options = pymunk.pygame_util.DrawOptions(surface)\n",
        "        >>> space.debug_draw(options)\n",
        "\n",
        "        You can control the color of a shape by setting shape.color to the color\n",
        "        you want it drawn in::\n",
        "\n",
        "        >>> c = pymunk.Circle(None, 10)\n",
        "        >>> c.color = pygame.Color(\"pink\")\n",
        "\n",
        "        See pygame_util.demo.py for a full example\n",
        "\n",
        "        Since pygame uses a coordiante system where y points down (in contrast\n",
        "        to many other cases), you either have to make the physics simulation\n",
        "        with Pymunk also behave in that way, or flip everything when you draw.\n",
        "\n",
        "        The easiest is probably to just make the simulation behave the same\n",
        "        way as Pygame does. In that way all coordinates used are in the same\n",
        "        orientation and easy to reason about::\n",
        "\n",
        "        >>> space = pymunk.Space()\n",
        "        >>> space.gravity = (0, -1000)\n",
        "        >>> body = pymunk.Body()\n",
        "        >>> body.position = (0, 0) # will be positioned in the top left corner\n",
        "        >>> space.debug_draw(options)\n",
        "\n",
        "        To flip the drawing its possible to set the module property\n",
        "        :py:data:`positive_y_is_up` to True. Then the pygame drawing will flip\n",
        "        the simulation upside down before drawing::\n",
        "\n",
        "        >>> positive_y_is_up = True\n",
        "        >>> body = pymunk.Body()\n",
        "        >>> body.position = (0, 0)\n",
        "        >>> # Body will be position in bottom left corner\n",
        "\n",
        "        :Parameters:\n",
        "                surface : pygame.Surface\n",
        "                    Surface that the objects will be drawn on\n",
        "        \"\"\"\n",
        "        self.surface = surface\n",
        "        super(DrawOptions, self).__init__()\n",
        "\n",
        "    def draw_circle(\n",
        "        self,\n",
        "        pos: Vec2d,\n",
        "        angle: float,\n",
        "        radius: float,\n",
        "        outline_color: SpaceDebugColor,\n",
        "        fill_color: SpaceDebugColor,\n",
        "    ) -> None:\n",
        "        p = to_pygame(pos, self.surface)\n",
        "\n",
        "        pygame.draw.circle(self.surface, fill_color.as_int(), p, round(radius), 0)\n",
        "        pygame.draw.circle(self.surface, light_color(fill_color).as_int(), p, round(radius-4), 0)\n",
        "\n",
        "        circle_edge = pos + Vec2d(radius, 0).rotated(angle)\n",
        "        p2 = to_pygame(circle_edge, self.surface)\n",
        "        line_r = 2 if radius > 20 else 1\n",
        "        # pygame.draw.lines(self.surface, outline_color.as_int(), False, [p, p2], line_r)\n",
        "\n",
        "    def draw_segment(self, a: Vec2d, b: Vec2d, color: SpaceDebugColor) -> None:\n",
        "        p1 = to_pygame(a, self.surface)\n",
        "        p2 = to_pygame(b, self.surface)\n",
        "\n",
        "        pygame.draw.aalines(self.surface, color.as_int(), False, [p1, p2])\n",
        "\n",
        "    def draw_fat_segment(\n",
        "        self,\n",
        "        a: Tuple[float, float],\n",
        "        b: Tuple[float, float],\n",
        "        radius: float,\n",
        "        outline_color: SpaceDebugColor,\n",
        "        fill_color: SpaceDebugColor,\n",
        "    ) -> None:\n",
        "        p1 = to_pygame(a, self.surface)\n",
        "        p2 = to_pygame(b, self.surface)\n",
        "\n",
        "        r = round(max(1, radius * 2))\n",
        "        pygame.draw.lines(self.surface, fill_color.as_int(), False, [p1, p2], r)\n",
        "        if r > 2:\n",
        "            orthog = [abs(p2[1] - p1[1]), abs(p2[0] - p1[0])]\n",
        "            if orthog[0] == 0 and orthog[1] == 0:\n",
        "                return\n",
        "            scale = radius / (orthog[0] * orthog[0] + orthog[1] * orthog[1]) ** 0.5\n",
        "            orthog[0] = round(orthog[0] * scale)\n",
        "            orthog[1] = round(orthog[1] * scale)\n",
        "            points = [\n",
        "                (p1[0] - orthog[0], p1[1] - orthog[1]),\n",
        "                (p1[0] + orthog[0], p1[1] + orthog[1]),\n",
        "                (p2[0] + orthog[0], p2[1] + orthog[1]),\n",
        "                (p2[0] - orthog[0], p2[1] - orthog[1]),\n",
        "            ]\n",
        "            pygame.draw.polygon(self.surface, fill_color.as_int(), points)\n",
        "            pygame.draw.circle(\n",
        "                self.surface,\n",
        "                fill_color.as_int(),\n",
        "                (round(p1[0]), round(p1[1])),\n",
        "                round(radius),\n",
        "            )\n",
        "            pygame.draw.circle(\n",
        "                self.surface,\n",
        "                fill_color.as_int(),\n",
        "                (round(p2[0]), round(p2[1])),\n",
        "                round(radius),\n",
        "            )\n",
        "\n",
        "    def draw_polygon(\n",
        "        self,\n",
        "        verts: Sequence[Tuple[float, float]],\n",
        "        radius: float,\n",
        "        outline_color: SpaceDebugColor,\n",
        "        fill_color: SpaceDebugColor,\n",
        "    ) -> None:\n",
        "        ps = [to_pygame(v, self.surface) for v in verts]\n",
        "        ps += [ps[0]]\n",
        "\n",
        "        radius = 2\n",
        "        pygame.draw.polygon(self.surface, light_color(fill_color).as_int(), ps)\n",
        "\n",
        "        if radius > 0:\n",
        "            for i in range(len(verts)):\n",
        "                a = verts[i]\n",
        "                b = verts[(i + 1) % len(verts)]\n",
        "                self.draw_fat_segment(a, b, radius, fill_color, fill_color)\n",
        "\n",
        "    def draw_dot(\n",
        "        self, size: float, pos: Tuple[float, float], color: SpaceDebugColor\n",
        "    ) -> None:\n",
        "        p = to_pygame(pos, self.surface)\n",
        "        pygame.draw.circle(self.surface, color.as_int(), p, round(size), 0)\n",
        "\n",
        "\n",
        "def pymunk_to_shapely(body, shapes):\n",
        "    geoms = list()\n",
        "    for shape in shapes:\n",
        "        if isinstance(shape, pymunk.shapes.Poly):\n",
        "            verts = [body.local_to_world(v) for v in shape.get_vertices()]\n",
        "            verts += [verts[0]]\n",
        "            geoms.append(sg.Polygon(verts))\n",
        "        else:\n",
        "            raise RuntimeError(f'Unsupported shape type {type(shape)}')\n",
        "    geom = sg.MultiPolygon(geoms)\n",
        "    return geom\n",
        "\n",
        "# env\n",
        "class PushTEnv(gym.Env): # Changed from gym.Env to gymnasium.Env\n",
        "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 10}\n",
        "    reward_range = (0., 1.)\n",
        "\n",
        "    def __init__(self,\n",
        "            legacy=False,\n",
        "            block_cog=None, damping=None,\n",
        "            render_action=True,\n",
        "            render_size=96,\n",
        "            reset_to_state=None\n",
        "        ):\n",
        "        super().__init__() # Added call to super().__init__()\n",
        "        self._seed = None\n",
        "        self.seed()\n",
        "        self.window_size = ws = 512  # The size of the PyGame window\n",
        "        self.render_size = render_size\n",
        "        self.sim_hz = 100\n",
        "        # Local controller params.\n",
        "        self.k_p, self.k_v = 100, 20    # PD control.z\n",
        "        self.control_hz = self.metadata['video.frames_per_second']\n",
        "        # legcay set_state for data compatiblity\n",
        "        self.legacy = legacy\n",
        "\n",
        "        # agent_pos, block_pos, block_angle\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0,0,0,0,0], dtype=np.float64),\n",
        "            high=np.array([ws,ws,ws,ws,np.pi*2], dtype=np.float64),\n",
        "            shape=(5,),\n",
        "            dtype=np.float64\n",
        "        )\n",
        "\n",
        "        # positional goal for agent\n",
        "        self.action_space = spaces.Box(\n",
        "            low=np.array([0,0], dtype=np.float64),\n",
        "            high=np.array([ws,ws], dtype=np.float64),\n",
        "            shape=(2,),\n",
        "            dtype=np.float64\n",
        "        )\n",
        "\n",
        "        self.block_cog = block_cog\n",
        "        self.damping = damping\n",
        "        self.render_action = render_action\n",
        "\n",
        "        \"\"\"\n",
        "        If human-rendering is used, `self.window` will be a reference\n",
        "        to the window that we draw to. `self.clock` will be a clock that is used\n",
        "        to ensure that the environment is rendered at the correct framerate in\n",
        "        human-mode. They will remain `None` until human-mode is used for the\n",
        "        first time.\n",
        "        \"\"\"\n",
        "        self.window = None\n",
        "        self.clock = None\n",
        "        self.screen = None\n",
        "\n",
        "        self.space = None\n",
        "        self.teleop = None\n",
        "        self.render_buffer = None\n",
        "        self.latest_action = None\n",
        "        self.reset_to_state = reset_to_state\n",
        "\n",
        "    def _install_collision_handlers(self):\n",
        "        \"\"\"Register collision callbacks in a way that works across pymunk builds.\"\"\"\n",
        "        self.n_contact_points = 0\n",
        "\n",
        "        def _post_solve(arbiter, space, data):\n",
        "            # Only count contacts that involve the BLOCK (so you don't count every wall-wall etc.)\n",
        "            a, b = arbiter.shapes\n",
        "            if TYPE_BLOCK in (a.collision_type, b.collision_type):\n",
        "                cps = arbiter.contact_point_set\n",
        "                self.n_contact_points += len(getattr(cps, \"points\", []))\n",
        "            # no return needed for post_solve\n",
        "\n",
        "        # Try the most specific API first, then fall back gracefully\n",
        "        if hasattr(self.space, \"add_collision_handler\"):\n",
        "            # fire for block-vs-agent and block-vs-wall\n",
        "            h1 = self.space.add_collision_handler(TYPE_BLOCK, TYPE_AGENT)\n",
        "            h1.post_solve = _post_solve\n",
        "            h2 = self.space.add_collision_handler(TYPE_BLOCK, TYPE_WALL)\n",
        "            h2.post_solve = _post_solve\n",
        "\n",
        "        elif hasattr(self.space, \"add_wildcard_collision_handler\"):\n",
        "            # fire for any collision involving the block\n",
        "            h = self.space.add_wildcard_collision_handler(TYPE_BLOCK)\n",
        "            h.post_solve = _post_solve\n",
        "\n",
        "        elif hasattr(self.space, \"add_default_collision_handler\"):\n",
        "            # last resort: fire for all collisions, filter inside callback\n",
        "            h = self.space.add_default_collision_handler()\n",
        "            h.post_solve = _post_solve\n",
        "\n",
        "        else:\n",
        "            raise RuntimeError(\"No collision handler API found on pymunk.Space\")\n",
        "\n",
        "    def reset(self, seed=None, options=None): # Modified reset method signature for Gymnasium\n",
        "        super().reset(seed=seed) # Added call to super().reset()\n",
        "        self._seed = seed # Keep original seed logic if needed, or rely on super().reset()\n",
        "        self._setup()\n",
        "        if self.block_cog is not None:\n",
        "            self.block.center_of_gravity = self.block_cog\n",
        "        if self.damping is not None:\n",
        "            self.space.damping = self.damping\n",
        "\n",
        "        # use legacy RandomState for compatiblity\n",
        "        state = self.reset_to_state\n",
        "        if state is None:\n",
        "            rs = np.random.RandomState(seed=seed)\n",
        "            state = np.array([\n",
        "                rs.randint(50, 450), rs.randint(50, 450),\n",
        "                rs.randint(100, 400), rs.randint(100, 400),\n",
        "                rs.randn() * 2 * np.pi - np.pi\n",
        "                ])\n",
        "        self._set_state(state)\n",
        "\n",
        "        obs = self._get_obs()\n",
        "        info = self._get_info()\n",
        "        return obs, info\n",
        "\n",
        "    def _count_contacts_now(self) -> int:\n",
        "        \"\"\"Count agent–block contact points using current arbiters (no handlers needed).\"\"\"\n",
        "        count = 0\n",
        "        # Preferred path: iterate arbiters produced by the last space.step()\n",
        "        if hasattr(self.space, \"arbiters\"):\n",
        "            for arb in self.space.arbiters:\n",
        "                a, b = arb.shapes\n",
        "                bodies = (a.body, b.body)\n",
        "                if (self.agent in bodies) and (self.block in bodies):\n",
        "                    cps = arb.contact_point_set\n",
        "                    count += len(getattr(cps, \"points\", []))\n",
        "            return count\n",
        "\n",
        "        # Fallback path: query collisions against the block’s shapes (older/stripped builds)\n",
        "        try:\n",
        "            for s in self.block.shapes:\n",
        "                infos = self.space.shape_query(s)  # list of ShapeQueryInfo\n",
        "                for qi in infos:\n",
        "                    if getattr(qi, \"shape\", None) in getattr(self.agent, \"shapes\", []):\n",
        "                        cps = getattr(qi, \"contact_point_set\", None)\n",
        "                        if cps is not None:\n",
        "                            count += len(getattr(cps, \"points\", []))\n",
        "            return count\n",
        "        except Exception:\n",
        "            # If even this isn’t available, return 0 rather than crashing.\n",
        "            return 0\n",
        "\n",
        "    def step(self, action):\n",
        "        dt = 1.0 / self.sim_hz\n",
        "        self.n_contact_points = 0\n",
        "        n_steps = self.sim_hz // self.control_hz\n",
        "\n",
        "        if action is not None:\n",
        "            self.latest_action = action\n",
        "            for _ in range(n_steps):\n",
        "                # PD control\n",
        "                acceleration = self.k_p * (action - self.agent.position) + self.k_v * (Vec2d(0, 0) - self.agent.velocity)\n",
        "                self.agent.velocity += acceleration * dt\n",
        "\n",
        "                # Physics step\n",
        "                self.space.step(dt)\n",
        "\n",
        "                # Count contacts that occurred during this sub-step\n",
        "                self.n_contact_points += self._count_contacts_now()\n",
        "\n",
        "        # compute reward\n",
        "        goal_body = self._get_goal_pose_body(self.goal_pose)\n",
        "        goal_geom = pymunk_to_shapely(goal_body, self.block.shapes)\n",
        "        block_geom = pymunk_to_shapely(self.block, self.block.shapes)\n",
        "\n",
        "        intersection_area = goal_geom.intersection(block_geom).area\n",
        "        goal_area = goal_geom.area\n",
        "        coverage = intersection_area / goal_area\n",
        "        reward = np.clip(coverage / self.success_threshold, 0, 1)\n",
        "        done = coverage > self.success_threshold\n",
        "        terminated = done\n",
        "        truncated = done\n",
        "\n",
        "        observation = self._get_obs()\n",
        "        info = self._get_info()\n",
        "\n",
        "        return observation, reward, terminated, truncated, info\n",
        "\n",
        "    def render(self, mode):\n",
        "        return self._render_frame(mode)\n",
        "\n",
        "    def teleop_agent(self):\n",
        "        TeleopAgent = collections.namedtuple('TeleopAgent', ['act'])\n",
        "        def act(obs):\n",
        "            act = None\n",
        "            mouse_position = pymunk.pygame_util.from_pygame(Vec2d(*pygame.mouse.get_pos()), self.screen)\n",
        "            if self.teleop or (mouse_position - self.agent.position).length < 30:\n",
        "                self.teleop = True\n",
        "                act = mouse_position\n",
        "            return act\n",
        "        return TeleopAgent(act)\n",
        "\n",
        "    def _get_obs(self):\n",
        "        obs = np.array(\n",
        "            tuple(self.agent.position) \\\n",
        "            + tuple(self.block.position) \\\n",
        "            + (self.block.angle % (2 * np.pi),))\n",
        "        return obs\n",
        "\n",
        "    def _get_goal_pose_body(self, pose):\n",
        "        mass = 1\n",
        "        inertia = pymunk.moment_for_box(mass, (50, 100))\n",
        "        body = pymunk.Body(mass, inertia)\n",
        "        # preserving the legacy assignment order for compatibility\n",
        "        # the order here dosn't matter somehow, maybe because CoM is aligned with body origin\n",
        "        body.position = pose[:2].tolist()\n",
        "        body.angle = pose[2]\n",
        "        return body\n",
        "\n",
        "    def _get_info(self):\n",
        "        n_steps = self.sim_hz // self.control_hz\n",
        "        n_contact_points_per_step = int(np.ceil(self.n_contact_points / n_steps))\n",
        "        info = {\n",
        "            'pos_agent': np.array(self.agent.position),\n",
        "            'vel_agent': np.array(self.agent.velocity),\n",
        "            'block_pose': np.array(list(self.block.position) + [self.block.angle]),\n",
        "            'goal_pose': self.goal_pose,\n",
        "            'n_contacts': n_contact_points_per_step}\n",
        "        return info\n",
        "\n",
        "    def _render_frame(self, mode):\n",
        "\n",
        "        if self.window is None and mode == \"human\":\n",
        "            pygame.init()\n",
        "            pygame.display.init()\n",
        "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
        "        if self.clock is None and mode == \"human\":\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
        "        canvas.fill((255, 255, 255))\n",
        "        self.screen = canvas\n",
        "\n",
        "        draw_options = DrawOptions(canvas)\n",
        "\n",
        "        # Draw goal pose.\n",
        "        goal_body = self._get_goal_pose_body(self.goal_pose)\n",
        "        for shape in self.block.shapes:\n",
        "            goal_points = [pymunk.pygame_util.to_pygame(goal_body.local_to_world(v), draw_options.surface) for v in shape.get_vertices()]\n",
        "            goal_points += [goal_points[0]]\n",
        "            pygame.draw.polygon(canvas, self.goal_color, goal_points)\n",
        "\n",
        "        # Draw agent and block.\n",
        "        self.space.debug_draw(draw_options)\n",
        "\n",
        "        if mode == \"human\":\n",
        "            # The following line copies our drawings from `canvas` to the visible window\n",
        "            self.window.blit(canvas, canvas.get_rect())\n",
        "            pygame.event.pump()\n",
        "            pygame.display.update()\n",
        "\n",
        "            # the clock is aleady ticked during in step for \"human\"\n",
        "\n",
        "\n",
        "        img = np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
        "            )\n",
        "        img = cv2.resize(img, (self.render_size, self.render_size))\n",
        "        if self.render_action:\n",
        "            if self.render_action and (self.latest_action is not None):\n",
        "                action = np.array(self.latest_action)\n",
        "                coord = (action / 512 * 96).astype(np.int32)\n",
        "                marker_size = int(8/96*self.render_size)\n",
        "                thickness = int(1/96*self.render_size)\n",
        "                cv2.drawMarker(img, coord,\n",
        "                    color=(255,0,0), markerType=cv2.MARKER_CROSS,\n",
        "                    markerSize=marker_size, thickness=thickness)\n",
        "        return img\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        if self.window is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        # Gymnasium handles seeding in reset(), so this method might not be strictly necessary\n",
        "        # depending on how it's used in the rest of the code.\n",
        "        # For now, keeping it to maintain compatibility with potential external calls.\n",
        "        if seed is None:\n",
        "            seed = np.random.randint(0,25536)\n",
        "        self._seed = seed\n",
        "        self.np_random = np.random.default_rng(seed)\n",
        "\n",
        "    def _handle_collision(self, arbiter, space, data):\n",
        "        self.n_contact_points += len(arbiter.contact_point_set.points)\n",
        "\n",
        "    def _set_state(self, state):\n",
        "        if isinstance(state, np.ndarray):\n",
        "            state = state.tolist()\n",
        "        pos_agent = state[:2]\n",
        "        pos_block = state[2:4]\n",
        "        rot_block = state[4]\n",
        "        self.agent.position = pos_agent\n",
        "        # setting angle rotates with respect to center of mass\n",
        "        # therefore will modify the geometric position\n",
        "        # if not the same as CoM\n",
        "        # therefore should be modified first.\n",
        "        if self.legacy:\n",
        "            # for compatiblity with legacy data\n",
        "            self.block.position = pos_block\n",
        "            self.block.angle = rot_block\n",
        "        else:\n",
        "            self.block.angle = rot_block\n",
        "            self.block.position = pos_block\n",
        "\n",
        "        # Run physics to take effect\n",
        "        self.space.step(1.0 / self.sim_hz)\n",
        "\n",
        "    def _set_state_local(self, state_local):\n",
        "        agent_pos_local = state_local[:2]\n",
        "        block_pose_local = state_local[2:]\n",
        "        tf_img_obj = st.AffineTransform(\n",
        "            translation=self.goal_pose[:2],\n",
        "            rotation=self.goal_pose[2])\n",
        "        tf_obj_new = st.AffineTransform(\n",
        "            translation=block_pose_local[:2],\n",
        "            rotation=block_pose_local[2]\n",
        "        )\n",
        "        tf_img_new = st.AffineTransform(\n",
        "            matrix=tf_img_obj.params @ tf_obj_new.params\n",
        "        )\n",
        "        agent_pos_new = tf_img_new(agent_pos_local)\n",
        "        new_state = np.array(\n",
        "            list(agent_pos_new[0]) + list(tf_img_new.translation) \\\n",
        "                + [tf_img_new.rotation])\n",
        "        self._set_state(new_state)\n",
        "        return new_state\n",
        "\n",
        "    def _setup(self):\n",
        "        self.space = pymunk.Space()\n",
        "        self.space.gravity = 0, 0\n",
        "        self.space.damping = 0\n",
        "        self.teleop = False\n",
        "        self.render_buffer = list()\n",
        "\n",
        "        # Add walls.\n",
        "        walls = [\n",
        "            self._add_segment((5, 506), (5, 5), 2),\n",
        "            self._add_segment((5, 5), (506, 5), 2),\n",
        "            self._add_segment((506, 5), (506, 506), 2),\n",
        "            self._add_segment((5, 506), (506, 506), 2)\n",
        "        ]\n",
        "        self.space.add(*walls)\n",
        "\n",
        "        # Add agent, block, and goal zone.\n",
        "        self.agent = self.add_circle((256, 400), 15)\n",
        "        self.block = self.add_tee((256, 300), 0)\n",
        "        self.goal_color = pygame.Color('LightGreen')\n",
        "        self.goal_pose = np.array([256,256,np.pi/4])  # x, y, theta (in radians)\n",
        "\n",
        "        # --- new, version-agnostic collision install ---\n",
        "        #self._install_collision_handlers()\n",
        "\n",
        "        self.n_contact_points = 0\n",
        "\n",
        "        self.max_score = 50 * 100\n",
        "        self.success_threshold = 0.95    # 95% coverage.\n",
        "\n",
        "    def _add_segment(self, a, b, radius):\n",
        "        shape = pymunk.Segment(self.space.static_body, a, b, radius)\n",
        "        shape.color = pygame.Color('LightGray')\n",
        "        shape.collision_type = TYPE_WALL          # <--- add this\n",
        "        return shape\n",
        "\n",
        "    def add_circle(self, position, radius):\n",
        "        body = pymunk.Body(body_type=pymunk.Body.KINEMATIC)\n",
        "        body.position = position\n",
        "        body.friction = 1\n",
        "        shape = pymunk.Circle(body, radius)\n",
        "        shape.color = pygame.Color('RoyalBlue')\n",
        "        shape.collision_type = TYPE_AGENT\n",
        "        self.space.add(body, shape)\n",
        "        return body\n",
        "\n",
        "    def add_box(self, position, height, width):\n",
        "        mass = 1\n",
        "        inertia = pymunk.moment_for_box(mass, (height, width))\n",
        "        body = pymunk.Body(mass, inertia)\n",
        "        body.position = position\n",
        "        shape = pymunk.Poly.create_box(body, (height, width))\n",
        "        shape.color = pygame.Color('LightSlateGray')\n",
        "        self.space.add(body, shape)\n",
        "        return body\n",
        "\n",
        "    def add_tee(self, position, angle, scale=30, color='LightSlateGray', mask=pymunk.ShapeFilter.ALL_MASKS()):\n",
        "        mass = 1\n",
        "        length = 4\n",
        "        vertices1 = [(-length*scale/2, scale),\n",
        "                                 ( length*scale/2, scale),\n",
        "                                 ( length*scale/2, 0),\n",
        "                                 (-length*scale/2, 0)]\n",
        "        inertia1 = pymunk.moment_for_poly(mass, vertices=vertices1)\n",
        "        vertices2 = [(-scale/2, scale),\n",
        "                                 (-scale/2, length*scale),\n",
        "                                 ( scale/2, length*scale),\n",
        "                                 ( scale/2, scale)]\n",
        "        inertia2 = pymunk.moment_for_poly(mass, vertices=vertices1)\n",
        "        body = pymunk.Body(mass, inertia1 + inertia2)\n",
        "        shape1 = pymunk.Poly(body, vertices1)\n",
        "        shape2 = pymunk.Poly(body, vertices2)\n",
        "        shape1.color = pygame.Color(color)\n",
        "        shape2.color = pygame.Color(color)\n",
        "        shape1.filter = pymunk.ShapeFilter(mask=mask)\n",
        "        shape2.filter = pymunk.ShapeFilter(mask=mask)\n",
        "        shape1.collision_type = TYPE_BLOCK\n",
        "        shape2.collision_type = TYPE_BLOCK\n",
        "        body.center_of_gravity = (shape1.center_of_gravity + shape2.center_of_gravity) / 2\n",
        "        body.position = position\n",
        "        body.angle = angle\n",
        "        body.friction = 1\n",
        "        self.space.add(body, shape1, shape2)\n",
        "        return body"
      ],
      "metadata": {
        "id": "L5E-nR6ornyg",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub.utils import DEFAULT_IGNORE_PATTERNS\n",
        "#@markdown ### **Env Demo**\n",
        "#@markdown Standard Gym Env (0.21.0 API)\n",
        "\n",
        "# 0. create env object\n",
        "env = PushTEnv()\n",
        "\n",
        "# 1. seed env for initial state.\n",
        "# Seed 0-200 are used for the demonstration dataset.\n",
        "seed = 1000\n",
        "# env.seed(1000) # Seed is now passed to reset\n",
        "\n",
        "# 2. must reset before use\n",
        "obs, info = env.reset(seed=seed)\n",
        "\n",
        "# 3. 2D positional action space [0,512]\n",
        "action = env.action_space.sample()\n",
        "\n",
        "# 4. Standard gym step method\n",
        "obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "# prints and explains each dimension of the observation and action vectors\n",
        "with np.printoptions(precision=4, suppress=True, threshold=5):\n",
        "    print(\"Obs: \", repr(obs))\n",
        "    print(\"Obs:        [agent_x,  agent_y,  block_x,  block_y,    block_angle]\")\n",
        "    print(\"Action: \", repr(action))\n",
        "    print(\"Action:   [target_agent_x, target_agent_y]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggSM9LOlpE45",
        "outputId": "4acaca8a-cb96-4c20-95aa-07ad3f420747"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obs:  array([140.7365, 232.0245, 292.    , 351.    ,   2.9196])\n",
            "Obs:        [agent_x,  agent_y,  block_x,  block_y,    block_angle]\n",
            "Action:  array([149.6372, 496.4991])\n",
            "Action:   [target_agent_x, target_agent_y]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Dataset**\n",
        "#@markdown\n",
        "#@markdown Defines `PushTStateDataset` and helper functions\n",
        "#@markdown\n",
        "#@markdown The dataset class\n",
        "#@markdown - Load data (obs, action) from a zarr storage\n",
        "#@markdown - Normalizes each dimension of obs and action to [-1,1]\n",
        "#@markdown - Returns\n",
        "#@markdown  - All possible segments with length `pred_horizon`\n",
        "#@markdown  - Pads the beginning and the end of each episode with repetition\n",
        "#@markdown  - key `obs`: shape (obs_horizon, obs_dim)\n",
        "#@markdown  - key `action`: shape (pred_horizon, action_dim)\n",
        "\n",
        "def create_sample_indices(\n",
        "        episode_ends:np.ndarray, sequence_length:int,\n",
        "        pad_before: int=0, pad_after: int=0):\n",
        "    indices = list()\n",
        "    for i in range(len(episode_ends)):\n",
        "        start_idx = 0\n",
        "        if i > 0:\n",
        "            start_idx = episode_ends[i-1]\n",
        "        end_idx = episode_ends[i]\n",
        "        episode_length = end_idx - start_idx\n",
        "\n",
        "        min_start = -pad_before\n",
        "        max_start = episode_length - sequence_length + pad_after\n",
        "\n",
        "        # range stops one idx before end\n",
        "        for idx in range(min_start, max_start+1):\n",
        "            buffer_start_idx = max(idx, 0) + start_idx\n",
        "            buffer_end_idx = min(idx+sequence_length, episode_length) + start_idx\n",
        "            start_offset = buffer_start_idx - (idx+start_idx)\n",
        "            end_offset = (idx+sequence_length+start_idx) - buffer_end_idx\n",
        "            sample_start_idx = 0 + start_offset\n",
        "            sample_end_idx = sequence_length - end_offset\n",
        "            indices.append([\n",
        "                buffer_start_idx, buffer_end_idx,\n",
        "                sample_start_idx, sample_end_idx])\n",
        "    indices = np.array(indices)\n",
        "    return indices\n",
        "\n",
        "\n",
        "def sample_sequence(train_data, sequence_length,\n",
        "                    buffer_start_idx, buffer_end_idx,\n",
        "                    sample_start_idx, sample_end_idx):\n",
        "    result = dict()\n",
        "    for key, input_arr in train_data.items():\n",
        "        sample = input_arr[buffer_start_idx:buffer_end_idx]\n",
        "        data = sample\n",
        "        if (sample_start_idx > 0) or (sample_end_idx < sequence_length):\n",
        "            data = np.zeros(\n",
        "                shape=(sequence_length,) + input_arr.shape[1:],\n",
        "                dtype=input_arr.dtype)\n",
        "            if sample_start_idx > 0:\n",
        "                data[:sample_start_idx] = sample[0]\n",
        "            if sample_end_idx < sequence_length:\n",
        "                data[sample_end_idx:] = sample[-1]\n",
        "            data[sample_start_idx:sample_end_idx] = sample\n",
        "        result[key] = data\n",
        "    return result\n",
        "\n",
        "# normalize data\n",
        "def get_data_stats(data):\n",
        "    data = data.reshape(-1,data.shape[-1])\n",
        "    stats = {\n",
        "        'min': np.min(data, axis=0),\n",
        "        'max': np.max(data, axis=0)\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "def normalize_data(data, stats):\n",
        "    # nomalize to [0,1]\n",
        "    ndata = (data - stats['min']) / (stats['max'] - stats['min'])\n",
        "    # normalize to [-1, 1]\n",
        "    ndata = ndata * 2 - 1\n",
        "    return ndata\n",
        "\n",
        "def unnormalize_data(ndata, stats):\n",
        "    ndata = (ndata + 1) / 2\n",
        "    data = ndata * (stats['max'] - stats['min']) + stats['min']\n",
        "    return data\n",
        "\n",
        "# dataset\n",
        "class PushTStateDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset_path,\n",
        "                 pred_horizon, obs_horizon, action_horizon):\n",
        "\n",
        "        # read from zarr dataset\n",
        "        from zarr.storage import ZipStore\n",
        "\n",
        "        # read from zarr dataset (zip in-place)\n",
        "        store = ZipStore(dataset_path, mode=\"r\")  # dataset_path is the .zip file\n",
        "        dataset_root = zarr.open_group(store=store, mode=\"r\")\n",
        "        # All demonstration episodes are concatinated in the first dimension N\n",
        "        train_data = {\n",
        "            # (N, action_dim)\n",
        "            'action': dataset_root['data']['action'][:],\n",
        "            # (N, obs_dim)\n",
        "            'obs': dataset_root['data']['state'][:]\n",
        "        }\n",
        "        # Marks one-past the last index for each episode\n",
        "        episode_ends = dataset_root['meta']['episode_ends'][:]\n",
        "\n",
        "        # compute start and end of each state-action sequence\n",
        "        # also handles padding\n",
        "        indices = create_sample_indices(\n",
        "            episode_ends=episode_ends,\n",
        "            sequence_length=pred_horizon,\n",
        "            # add padding such that each timestep in the dataset are seen\n",
        "            pad_before=obs_horizon-1,\n",
        "            pad_after=action_horizon-1)\n",
        "\n",
        "        # compute statistics and normalized data to [-1,1]\n",
        "        stats = dict()\n",
        "        normalized_train_data = dict()\n",
        "        for key, data in train_data.items():\n",
        "            stats[key] = get_data_stats(data)\n",
        "            normalized_train_data[key] = normalize_data(data, stats[key])\n",
        "\n",
        "        self.indices = indices\n",
        "        self.stats = stats\n",
        "        self.normalized_train_data = normalized_train_data\n",
        "        self.pred_horizon = pred_horizon\n",
        "        self.action_horizon = action_horizon\n",
        "        self.obs_horizon = obs_horizon\n",
        "\n",
        "    def __len__(self):\n",
        "        # all possible segments of the dataset\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get the start/end indices for this datapoint\n",
        "        buffer_start_idx, buffer_end_idx, \\\n",
        "            sample_start_idx, sample_end_idx = self.indices[idx]\n",
        "\n",
        "        # get nomralized data using these indices\n",
        "        nsample = sample_sequence(\n",
        "            train_data=self.normalized_train_data,\n",
        "            sequence_length=self.pred_horizon,\n",
        "            buffer_start_idx=buffer_start_idx,\n",
        "            buffer_end_idx=buffer_end_idx,\n",
        "            sample_start_idx=sample_start_idx,\n",
        "            sample_end_idx=sample_end_idx\n",
        "        )\n",
        "\n",
        "        # discard unused observations\n",
        "        nsample['obs'] = nsample['obs'][:self.obs_horizon,:]\n",
        "        return nsample\n"
      ],
      "metadata": {
        "id": "vHepJOFBucwg",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Dataset Demo**\n",
        "\n",
        "# download demonstration data from Google Drive\n",
        "dataset_path = \"pusht_cchi_v7_replay.zarr.zip\"\n",
        "if not os.path.isfile(dataset_path):\n",
        "    id = \"1KY1InLurpMvJDRb14L9NlXT_fEsCvVUq&confirm=t\"\n",
        "    gdown.download(id=id, output=dataset_path, quiet=False)\n",
        "\n",
        "# parameters\n",
        "pred_horizon = 16\n",
        "obs_horizon = 2\n",
        "action_horizon = 8\n",
        "#|o|o|                             observations: 2\n",
        "#| |a|a|a|a|a|a|a|a|               actions executed: 8\n",
        "#|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p| actions predicted: 16\n",
        "\n",
        "# create dataset from file\n",
        "dataset = PushTStateDataset(\n",
        "    dataset_path=dataset_path,\n",
        "    pred_horizon=pred_horizon,\n",
        "    obs_horizon=obs_horizon,\n",
        "    action_horizon=action_horizon\n",
        ")\n",
        "# save training data statistics (min, max) for each dim\n",
        "stats = dataset.stats\n",
        "\n",
        "# create dataloader\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=256,\n",
        "    num_workers=1,\n",
        "    shuffle=True,\n",
        "    # accelerate cpu-gpu transfer\n",
        "    pin_memory=True,\n",
        "    # don't kill worker process afte each epoch\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "# visualize data in batch\n",
        "batch = next(iter(dataloader))\n",
        "print(\"batch['obs'].shape:\", batch['obs'].shape)\n",
        "print(\"batch['action'].shape\", batch['action'].shape)"
      ],
      "metadata": {
        "id": "9ZiHF3lzvB6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae9061c8-fcb5-40ea-c93e-3b45b47e5bf0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KY1InLurpMvJDRb14L9NlXT_fEsCvVUq&confirm=t\n",
            "To: /content/pusht_cchi_v7_replay.zarr.zip\n",
            "100%|██████████| 31.1M/31.1M [00:00<00:00, 59.2MB/s]\n",
            "/usr/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=1407) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch['obs'].shape: torch.Size([256, 2, 5])\n",
            "batch['action'].shape torch.Size([256, 16, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown ### **Network**\n",
        "# @markdown\n",
        "# @markdown Defines a 1D UNet architecture `ConditionalUnet1D`\n",
        "# @markdown as the noise prediction network\n",
        "# @markdown\n",
        "# @markdown Components\n",
        "# @markdown - `SinusoidalPosEmb` Positional encoding for the diffusion iteration k\n",
        "# @markdown - `Downsample1d` Strided convolution to reduce temporal resolution\n",
        "# @markdown - `Upsample1d` Transposed convolution to increase temporal resolution\n",
        "# @markdown - `Conv1dBlock` Conv1d --> GroupNorm --> Mish\n",
        "# @markdown - `ConditionalResidualBlock1D` Takes two inputs `x` and `cond`. \\\n",
        "# @markdown `x` is passed through 2 `Conv1dBlock` stacked together with residual connection.\n",
        "# @markdown `cond` is applied to `x` with [FiLM](https://arxiv.org/abs/1709.07871) conditioning.\n",
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = math.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class Downsample1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class Upsample1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class Conv1dBlock(nn.Module):\n",
        "    '''\n",
        "        Conv1d --> GroupNorm --> Mish\n",
        "    '''\n",
        "\n",
        "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
        "            nn.GroupNorm(n_groups, out_channels),\n",
        "            nn.Mish(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class ConditionalResidualBlock1D(nn.Module):\n",
        "    def __init__(self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            cond_dim,\n",
        "            kernel_size=3,\n",
        "            n_groups=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),\n",
        "            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),\n",
        "        ])\n",
        "\n",
        "        # FiLM modulation https://arxiv.org/abs/1709.07871\n",
        "        # predicts per-channel scale and bias\n",
        "        cond_channels = out_channels * 2\n",
        "        self.out_channels = out_channels\n",
        "        self.cond_encoder = nn.Sequential(\n",
        "            nn.Mish(),\n",
        "            nn.Linear(cond_dim, cond_channels),\n",
        "            nn.Unflatten(-1, (-1, 1))\n",
        "        )\n",
        "\n",
        "        # make sure dimensions compatible\n",
        "        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \\\n",
        "            if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        '''\n",
        "            x : [ batch_size x in_channels x horizon ]\n",
        "            cond : [ batch_size x cond_dim]\n",
        "\n",
        "            returns:\n",
        "            out : [ batch_size x out_channels x horizon ]\n",
        "        '''\n",
        "        out = self.blocks[0](x)\n",
        "        embed = self.cond_encoder(cond)\n",
        "\n",
        "        embed = embed.reshape(\n",
        "            embed.shape[0], 2, self.out_channels, 1)\n",
        "        scale = embed[:,0,...]\n",
        "        bias = embed[:,1,...]\n",
        "        out = scale * out + bias\n",
        "\n",
        "        out = self.blocks[1](out)\n",
        "        out = out + self.residual_conv(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConditionalUnet1D(nn.Module):\n",
        "    def __init__(self,\n",
        "        input_dim,\n",
        "        global_cond_dim,\n",
        "        diffusion_step_embed_dim=256,\n",
        "        down_dims=[256,512,1024],\n",
        "        kernel_size=5,\n",
        "        n_groups=8\n",
        "        ):\n",
        "        \"\"\"\n",
        "        input_dim: Dim of actions.\n",
        "        global_cond_dim: Dim of global conditioning applied with FiLM\n",
        "          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim\n",
        "        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k\n",
        "        down_dims: Channel size for each UNet level.\n",
        "          The length of this array determines numebr of levels.\n",
        "        kernel_size: Conv kernel size\n",
        "        n_groups: Number of groups for GroupNorm\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        all_dims = [input_dim] + list(down_dims)\n",
        "        start_dim = down_dims[0]\n",
        "\n",
        "        dsed = diffusion_step_embed_dim\n",
        "        diffusion_step_encoder = nn.Sequential(\n",
        "            SinusoidalPosEmb(dsed),\n",
        "            nn.Linear(dsed, dsed * 4),\n",
        "            nn.Mish(),\n",
        "            nn.Linear(dsed * 4, dsed),\n",
        "        )\n",
        "        cond_dim = dsed + global_cond_dim\n",
        "\n",
        "        in_out = list(zip(all_dims[:-1], all_dims[1:]))\n",
        "        mid_dim = all_dims[-1]\n",
        "        self.mid_modules = nn.ModuleList([\n",
        "            ConditionalResidualBlock1D(\n",
        "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
        "                kernel_size=kernel_size, n_groups=n_groups\n",
        "            ),\n",
        "            ConditionalResidualBlock1D(\n",
        "                mid_dim, mid_dim, cond_dim=cond_dim,\n",
        "                kernel_size=kernel_size, n_groups=n_groups\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "        down_modules = nn.ModuleList([])\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (len(in_out) - 1)\n",
        "            down_modules.append(nn.ModuleList([\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_in, dim_out, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_out, dim_out, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                Downsample1d(dim_out) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "\n",
        "        up_modules = nn.ModuleList([])\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "            is_last = ind >= (len(in_out) - 1)\n",
        "            up_modules.append(nn.ModuleList([\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_out*2, dim_in, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                ConditionalResidualBlock1D(\n",
        "                    dim_in, dim_in, cond_dim=cond_dim,\n",
        "                    kernel_size=kernel_size, n_groups=n_groups),\n",
        "                Upsample1d(dim_in) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "\n",
        "        final_conv = nn.Sequential(\n",
        "            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),\n",
        "            nn.Conv1d(start_dim, input_dim, 1),\n",
        "        )\n",
        "\n",
        "        self.diffusion_step_encoder = diffusion_step_encoder\n",
        "        self.up_modules = up_modules\n",
        "        self.down_modules = down_modules\n",
        "        self.final_conv = final_conv\n",
        "\n",
        "        print(\"number of parameters: {:e}\".format(\n",
        "            sum(p.numel() for p in self.parameters()))\n",
        "        )\n",
        "\n",
        "    def forward(self,\n",
        "            sample: torch.Tensor,\n",
        "            timestep: Union[torch.Tensor, float, int],\n",
        "            global_cond=None):\n",
        "        \"\"\"\n",
        "        x: (B,T,input_dim)\n",
        "        timestep: (B,) or int, diffusion step\n",
        "        global_cond: (B,global_cond_dim)\n",
        "        output: (B,T,input_dim)\n",
        "        \"\"\"\n",
        "        # (B,T,C)\n",
        "        sample = sample.moveaxis(-1,-2)\n",
        "        # (B,C,T)\n",
        "\n",
        "        # 1. time\n",
        "        timesteps = timestep\n",
        "        if not torch.is_tensor(timesteps):\n",
        "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
        "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
        "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
        "            timesteps = timesteps[None].to(sample.device)\n",
        "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
        "        timesteps = timesteps.expand(sample.shape[0])\n",
        "\n",
        "        global_feature = self.diffusion_step_encoder(timesteps)\n",
        "\n",
        "        if global_cond is not None:\n",
        "            global_feature = torch.cat([\n",
        "                global_feature, global_cond\n",
        "            ], axis=-1)\n",
        "\n",
        "        x = sample\n",
        "        h = []\n",
        "        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):\n",
        "            x = resnet(x, global_feature)\n",
        "            x = resnet2(x, global_feature)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        for mid_module in self.mid_modules:\n",
        "            x = mid_module(x, global_feature)\n",
        "\n",
        "        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = resnet(x, global_feature)\n",
        "            x = resnet2(x, global_feature)\n",
        "            x = upsample(x)\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        # (B,C,T)\n",
        "        x = x.moveaxis(-1,-2)\n",
        "        # (B,T,C)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "X-XRB_g3vsgf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Network Demo**\n",
        "\n",
        "# observation and action dimensions corrsponding to\n",
        "# the output of PushTEnv\n",
        "obs_dim = 5\n",
        "action_dim = 2\n",
        "\n",
        "# create network object\n",
        "noise_pred_net = ConditionalUnet1D(\n",
        "    input_dim=action_dim,\n",
        "    global_cond_dim=obs_dim*obs_horizon\n",
        ")\n",
        "\n",
        "# example inputs\n",
        "noised_action = torch.randn((1, pred_horizon, action_dim))\n",
        "obs = torch.zeros((1, obs_horizon, obs_dim))\n",
        "diffusion_iter = torch.zeros((1,))\n",
        "\n",
        "# the noise prediction network\n",
        "# takes noisy action, diffusion iteration and observation as input\n",
        "# predicts the noise added to action\n",
        "noise = noise_pred_net(\n",
        "    sample=noised_action,\n",
        "    timestep=diffusion_iter,\n",
        "    global_cond=obs.flatten(start_dim=1))\n",
        "\n",
        "# illustration of removing noise\n",
        "# the actual noise removal is performed by NoiseScheduler\n",
        "# and is dependent on the diffusion noise schedule\n",
        "denoised_action = noised_action - noise\n",
        "\n",
        "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
        "num_diffusion_iters = 100\n",
        "noise_scheduler = DDPMScheduler(\n",
        "    num_train_timesteps=num_diffusion_iters,\n",
        "    # the choise of beta schedule has big impact on performance\n",
        "    # we found squared cosine works the best\n",
        "    beta_schedule='squaredcos_cap_v2',\n",
        "    # clip output to [-1,1] to improve stability\n",
        "    clip_sample=True,\n",
        "    # our network predicts noise (instead of denoised action)\n",
        "    prediction_type='epsilon'\n",
        ")\n",
        "\n",
        "# device transfer\n",
        "device = torch.device('cuda')\n",
        "_ = noise_pred_net.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4APZkqh336-M",
        "outputId": "48664972-673e-4c14-9453-8f4c1a9a5173"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 6.535322e+07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Training**\n",
        "#@markdown\n",
        "#@markdown Takes about an hour. If you don't want to wait, skip to the next cell\n",
        "#@markdown to load pre-trained weights\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "# Exponential Moving Average\n",
        "# accelerates training and improves stability\n",
        "# holds a copy of the model weights\n",
        "ema = EMAModel(\n",
        "    parameters=noise_pred_net.parameters(),\n",
        "    power=0.75)\n",
        "\n",
        "# Standard ADAM optimizer\n",
        "# Note that EMA parametesr are not optimized\n",
        "optimizer = torch.optim.AdamW(\n",
        "    params=noise_pred_net.parameters(),\n",
        "    lr=1e-4, weight_decay=1e-6)\n",
        "\n",
        "# Cosine LR schedule with linear warmup\n",
        "lr_scheduler = get_scheduler(\n",
        "    name='cosine',\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=500,\n",
        "    num_training_steps=len(dataloader) * num_epochs\n",
        ")\n",
        "\n",
        "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
        "    # epoch loop\n",
        "    for epoch_idx in tglobal:\n",
        "        epoch_loss = list()\n",
        "        # batch loop\n",
        "        with tqdm(dataloader, desc='Batch', leave=False) as tepoch:\n",
        "            for nbatch in tepoch:\n",
        "                # data normalized in dataset\n",
        "                # device transfer\n",
        "                nobs = nbatch['obs'].to(device)\n",
        "                naction = nbatch['action'].to(device)\n",
        "                B = nobs.shape[0]\n",
        "\n",
        "                # observation as FiLM conditioning\n",
        "                # (B, obs_horizon, obs_dim)\n",
        "                obs_cond = nobs[:,:obs_horizon,:]\n",
        "                # (B, obs_horizon * obs_dim)\n",
        "                obs_cond = obs_cond.flatten(start_dim=1)\n",
        "\n",
        "                # sample noise to add to actions\n",
        "                noise = torch.randn(naction.shape, device=device)\n",
        "\n",
        "                # sample a diffusion iteration for each data point\n",
        "                timesteps = torch.randint(\n",
        "                    0, noise_scheduler.config.num_train_timesteps,\n",
        "                    (B,), device=device\n",
        "                ).long()\n",
        "\n",
        "                # add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
        "                # (this is the forward diffusion process)\n",
        "                noisy_actions = noise_scheduler.add_noise(\n",
        "                    naction, noise, timesteps)\n",
        "\n",
        "                # predict the noise residual\n",
        "                noise_pred = noise_pred_net(\n",
        "                    noisy_actions, timesteps, global_cond=obs_cond)\n",
        "\n",
        "                # L2 loss\n",
        "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
        "\n",
        "                # optimize\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                # step lr scheduler every batch\n",
        "                # this is different from standard pytorch behavior\n",
        "                lr_scheduler.step()\n",
        "\n",
        "                # update Exponential Moving Average of the model weights\n",
        "                ema.step(noise_pred_net.parameters())\n",
        "\n",
        "                # logging\n",
        "                loss_cpu = loss.item()\n",
        "                epoch_loss.append(loss_cpu)\n",
        "                tepoch.set_postfix(loss=loss_cpu)\n",
        "        tglobal.set_postfix(loss=np.mean(epoch_loss))\n",
        "\n",
        "# Weights of the EMA model\n",
        "# is used for inference\n",
        "ema_noise_pred_net = noise_pred_net\n",
        "ema.copy_to(ema_noise_pred_net.parameters())"
      ],
      "metadata": {
        "id": "93E9RdnR4D8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **Loading Pretrained Checkpoint**\n",
        "#@markdown Set `load_pretrained = True` to load pretrained weights.\n",
        "\n",
        "load_pretrained = True\n",
        "if load_pretrained:\n",
        "  ckpt_path = \"pusht_state_100ep.ckpt\"\n",
        "  if not os.path.isfile(ckpt_path):\n",
        "      id = \"1mHDr_DEZSdiGo9yecL50BBQYzR8Fjhl_&confirm=t\"\n",
        "      gdown.download(id=id, output=ckpt_path, quiet=False)\n",
        "\n",
        "  state_dict = torch.load(ckpt_path, map_location='cuda')\n",
        "  ema_noise_pred_net = noise_pred_net\n",
        "  ema_noise_pred_net.load_state_dict(state_dict)\n",
        "  print('Pretrained weights loaded.')\n",
        "else:\n",
        "  print(\"Skipped pretrained weight loading.\")"
      ],
      "metadata": {
        "id": "6F3hUbIuxGdO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c88dea-f2dd-4d35-80c7-b9036d0218ca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mHDr_DEZSdiGo9yecL50BBQYzR8Fjhl_&confirm=t\n",
            "To: /content/pusht_state_100ep.ckpt\n",
            "100%|██████████| 261M/261M [00:04<00:00, 59.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained weights loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a49ebc45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362,
          "referenced_widgets": [
            "ef0de47646ff4798bc109cb39d5498d3",
            "052e24aba89648b9ad5bdc91a91a63f0",
            "ee500ae747fe4dfe8dc87378999f78a1",
            "f2d0ffbc78624aa481574f067f924381",
            "728aa183d4e04e37bbf07a915d0ab8cb",
            "87369ac78d0d42568031a8de8a8dafe9",
            "fbdb972d50c24877b985899576781997",
            "a66baea8ff3e414183a9cd40601a4608",
            "0592bee52c1c49a0adfbc75b5d12ae16",
            "af26c7d9942c4750a6c475e7b06e28a4",
            "9fd57981de8b4200890ae131f82b6eff"
          ]
        },
        "outputId": "2c0a2917-3b31-42f6-a31e-a9f3a4eee628"
      },
      "source": [
        "#@markdown ### **Inference**\n",
        "\n",
        "# limit enviornment interaction to 200 steps before termination\n",
        "max_steps = 200\n",
        "env = PushTEnv()\n",
        "# use a seed >200 to avoid initial states seen in the training dataset\n",
        "env.seed(100000)\n",
        "\n",
        "# get first observation\n",
        "obs, info = env.reset()\n",
        "\n",
        "# keep a queue of last 2 steps of observations\n",
        "obs_deque = collections.deque(\n",
        "    [obs] * obs_horizon, maxlen=obs_horizon)\n",
        "# save visualization and rewards\n",
        "imgs = [env.render(mode='rgb_array')]\n",
        "rewards = list()\n",
        "done = False\n",
        "step_idx = 0\n",
        "\n",
        "with tqdm(total=max_steps, desc=\"Eval PushTStateEnv\") as pbar:\n",
        "    while not done:\n",
        "        B = 1\n",
        "        # stack the last obs_horizon (2) number of observations\n",
        "        obs_seq = np.stack(obs_deque)\n",
        "        # normalize observation\n",
        "        nobs = normalize_data(obs_seq, stats=stats['obs'])\n",
        "        # device transfer\n",
        "        nobs = torch.from_numpy(nobs).to(device, dtype=torch.float32)\n",
        "\n",
        "        # infer action\n",
        "        with torch.no_grad():\n",
        "            # reshape observation to (B,obs_horizon*obs_dim)\n",
        "            obs_cond = nobs.unsqueeze(0).flatten(start_dim=1)\n",
        "\n",
        "            # initialize action from Guassian noise\n",
        "            noisy_action = torch.randn(\n",
        "                (B, pred_horizon, action_dim), device=device)\n",
        "            naction = noisy_action\n",
        "\n",
        "            # init scheduler\n",
        "            noise_scheduler.set_timesteps(num_diffusion_iters)\n",
        "\n",
        "            for k in noise_scheduler.timesteps:\n",
        "                # predict noise\n",
        "                noise_pred = ema_noise_pred_net(\n",
        "                    sample=naction,\n",
        "                    timestep=k,\n",
        "                    global_cond=obs_cond\n",
        "                )\n",
        "\n",
        "                # inverse diffusion step (remove noise)\n",
        "                naction = noise_scheduler.step(\n",
        "                    model_output=noise_pred,\n",
        "                    timestep=k,\n",
        "                    sample=naction\n",
        "                ).prev_sample\n",
        "\n",
        "        # unnormalize action\n",
        "        naction = naction.detach().to('cpu').numpy()\n",
        "        # (B, pred_horizon, action_dim)\n",
        "        naction = naction[0]\n",
        "        action_pred = unnormalize_data(naction, stats=stats['action'])\n",
        "\n",
        "        # only take action_horizon number of actions\n",
        "        start = obs_horizon - 1\n",
        "        end = start + action_horizon\n",
        "        action = action_pred[start:end,:]\n",
        "        # (action_horizon, action_dim)\n",
        "\n",
        "        # execute action_horizon number of steps\n",
        "        # without replanning\n",
        "        for i in range(len(action)):\n",
        "            # stepping env\n",
        "            obs, reward, done, _, info = env.step(action[i])\n",
        "            # save observations\n",
        "            obs_deque.append(obs)\n",
        "            # and reward/vis\n",
        "            rewards.append(reward)\n",
        "            imgs.append(env.render(mode='rgb_array'))\n",
        "\n",
        "            # update progress bar\n",
        "            step_idx += 1\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix(reward=reward)\n",
        "            if step_idx > max_steps:\n",
        "                done = True\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "# print out the maximum target coverage\n",
        "print('Score: ', max(rewards))\n",
        "\n",
        "# visualize\n",
        "from IPython.display import Video\n",
        "vwrite('vis.mp4', imgs)\n",
        "Video('vis.mp4', embed=True, width=256, height=256)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Eval PushTStateEnv:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef0de47646ff4798bc109cb39d5498d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score:  0.9642183759788252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/skvideo/io/abstract.py:514: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  self._proc.stdin.write(vid.tostring())\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Video object>"
            ],
            "text/html": [
              "<video controls  width=\"256\"  height=\"256\">\n",
              " <source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAWvdtZGF0AAACrQYF//+p3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE2MyByMzA2MCA1ZGI2YWE2IC0gSC4yNjQvTVBFRy00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAyMSAtIGh0dHA6Ly93d3cudmlkZW9sYW4ub3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFseXNlPTB4MzoweDExMyBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVkX3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBkZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD00IHRocmVhZHM9MyBsb29rYWhlYWRfdGhyZWFkcz0xIHNsaWNlZF90aHJlYWRzPTAgbnI9MCBkZWNpbWF0ZT0xIGludGVybGFjZWQ9MCBibHVyYXlfY29tcGF0PTAgY29uc3RyYWluZWRfaW50cmE9MCBiZnJhbWVzPTMgYl9weXJhbWlkPTIgYl9hZGFwdD0xIGJfYmlhcz0wIGRpcmVjdD0xIHdlaWdodGI9MSBvcGVuX2dvcD0wIHdlaWdodHA9MiBrZXlpbnQ9MjUwIGtleWludF9taW49MjUgc2NlbmVjdXQ9NDAgaW50cmFfcmVmcmVzaD0wIHJjX2xvb2thaGVhZD00MCByYz1jcmYgbWJ0cmVlPTEgY3JmPTIzLjAgcWNvbXA9MC42MCBxcG1pbj0wIHFwbWF4PTY5IHFwc3RlcD00IGlwX3JhdGlvPTEuNDAgYXE9MToxLjAwAIAAAAJ5ZYiEAG+NcDEtbxi6VEm7ILG9lMf///vDT/wQ9b9b1F4tP8QqhTCbUZ6wBBR5TRilCnv/+1DtkUG27sWwyWGNwriPwh4KOmbGrxwJp3Op+sq4NANbumFfsmy+QrKmliJHUuzFk1NvcbZSd/HWCj2C1XlflLWBpmzw6sD4eEGyoNtWMqCICYApIfN9JTaUosDXY7R/EUIyIvwh4umYu2TPoDNqhKTiD+OzYV9rIquEDLjdW3kjsE+cAGHfYqz2u+6Kjuz7zyHG5LZ6GILahxKhVIIVZoiyAdTKNI4Zxy2VgqZRGE6KxZoeC7iNFO52jvLJnPt4tLcM2hDvvTOCHIWm9VWzeWBjhkzqETcMuXmY3vrQzLL0763eWZ/okgZ3d/Zqx1c/6xrJkTnPF9PCUE9YQiO4zIF3Y2WTfiIRCvfoTLBBjzWkQyxNRHQ6BUusDDommoSo4uk30hFlp0WbIKA8SCutjYRpUs7veitfPtW2b95zazQ4v0zrgplX70dIqy4jRht5kdZpGjSZkGfgCUbarTWAOBrJ7Mdr/YjTkwA53qKZZhlt43QWoKHFhzpIPz855J5Y2msBuJtI9vp/p2cj1ZG9hJvXyM6pdI/KgAEnwGEGU3JGajqL2vNdTYIyDKBW32mYdL82c8RiseXEnEdoKL40Q2rQynjPm1PB1ct0+cUuzH1fszWNozU2/d7qcduBVDPnpmp8CvmUp5y/zKN5vMtA8ZFNSyzN5eFzygfducuaAnWoBV5sIx9gp4gvjIPbDZy+6YEAAwto6NFhGlp/VIK31tzk3vl7/nSZxzqRYrcMdLJQsi69fzBBPeO4TXZRVQwD6g1lSuM9AAAAZUGaIWxG//p/C8UAAvDSGQAhtv+YRbJuTzJ4XV0YWd9mnYzbbyXZxoZmUee8xmQsqV/6cvr69fwi/1Ajunf8RhtOmlZOX9/7XUwyVhSSqsXS3aXfU6WMr2MA5taCkuhi6j7x18XAAAAAY0GaQzwhkymEb/qibLsMVC+AJ1/Nw9WGjQqY5/8rTOc3dNvvGZ1cfRC+wFs2s1coCd5HFIFPiJetF4JmtMjkT77/2cVb0FjGIxLxxaw4CQ0NB7SW5yZlHFp3gzZENTP/8Q2OKQAAACsBnmJqRH9nxRhpA9TCaxCB9xgV1oqeXYQfc9jrXrr3Jvis6NRPcVXbPg5AAAAAOkGaZEnhDyZTAjf/+paO+Y4AC+roS7kuYyHTpkpmmHWkLBE/X/x3Qu6UPPqo3bKMKf1/CIlSR39sOpUAAABFQZqGSeEPJlMFETxv+pP5hHcap3FvcACtkg3+q6uEAv4865UNwLs0z77bZIlZbt1lEkYEh1GGsYh0QL5+J/6hp6FBXwOBAAAAKwGepWpEf2gAhY6MSju8gpeP8/1wFwW15H6IU7tDPH2mE8nWj1v6SD6GDckAAAA9QZqnSeEPJlMCN//6nYpbH0o/RpIAI0XqP1UzyhoXBJxBDdrCxxBPE/iXHeIYcua84mU6efnx+ZiqhBOegQAAAGVBmspJ4Q8mUwI3//rf2UCBVID8sgfyNhzm8iM4m+N3hPjKvgZIdjQBVIcpj2RabXSBmSK8DWSJWP8Eby8Y+9T+ENhZ56a/N7zEkNU+JfqKJ0JHhxpjzXdQGkPFxS8SzBHbRCsQgAAAAClBnuhFETzfds2BBiIeNlPLki37sBiIAy2MtKdUInFO1CUnr6fT73vp+gAAACYBnwlqRH99Er8ae+ZIYrcz5k0TWzUkp3OiAL6Ig8ppF2d0ZsvemwAAAFRBmwtJqEFomUwI3/rRhfaEKoBRpPmbWiaJPjPoj2rG9f3zWcBIYd5+ODavtm8c2E9EpikEIro8241iRNd75zMvWJ7TGo18B+nqf25iT/y4Unuy2awAAABtQZssSeEKUmUwI3/7THS7SkxiIGOm11EXBoBdzRD2VMsqcpoCD89jt1C3QVrUCnGAX2uQcahoePc/xSQ2EV/6xKAARiJBGM/PSzVv2KRPOs167oYg9Kegkg3rT0esEUgnfMrP/CX2YEluquVvSAAAAEhBm01J4Q6JlMCN//tMdFjqST4cA8c4HW3LD+N9Zq1fQUUkBzKVqED4TMRt0kRc9WDUJPU5gmzPbj1UbjWjyQzZlf854l1zU/8AAACKQZtuSeEPJlMCN//7I9vgVgAV1FRZBa5ALZLb/1S//3cDv9dy/BHsGg4ZbHOU9hZRXE3fq3s3arwR+oyTIWG1V9pbNaerGqezK8FUYIo46fGuXVlrVY+fRFqCCbN0z1tRmTBiWWiobRFRffsGboR29/iE0rywZP/9738wfkszverAyuUtBGviXzftAAAAaEGbj0nhDyZTAjf/+yQzpQTjPojYe5UPM+b3A6FGae2vqXAXCErrruJIvwLkN+8yuiNmooah46Wb3/NrTl6mEJP1JlV5PaGsQ4MVWCsP91URT2l8MP29Zep3IBdsaGOPBLHdzXhBfFT5AAAA2EGbsknhDyZTAjf/+yNmXLz8EstnUbMQzZ21Xty0eMObLqftEbvO4yDa8VJp+BtYYGhonfLXu+cMU4WI/xX5rBhLEJboiEOyjL3T4QJAlmfVxDw464xH2ZTP/dPkF7QSS75pZlmPob5nqnMr+rnuQIf3ZUmTPCr63hovFzIpnHwYOrYke5BpIQIMV6Aq+EGjx3JNKDQyfIqEb5lowVPXbXVJRiFkxxmvo1qlX8GUuVjrsizHRT+4R/kiQKVWBPQyM1FXCe1cUonCniquUhK75RXIy8Yw2xLYlQAAAGZBn9BFETzfc4dN4kdxj2yHD/HPZuV2+ahDFxJhYxHLLpHoZN0Szf59MYIk7AZ9t7Q0rGYNBiKEhAja6W7Gn6RWXny3NEiZtSgHHN72UN7HQy8XegcZHz4bRJqarrHM5lB8wnA+K9gAAABmAZ/xakR/eGK5mehzGlfYULQi1rh7apipb0uqsifPIk779TNgOHrFyykrhysch+TZdjO99HvTACM9m2F4+vA8kYB0zfqMXW/SSYP7/v7fCLuazgclEA90r2AdSiwv9kaU0UeoTpC/AAAA5EGb9UmoQWiZTAjf+yy6aJysF7FUn+t9TpqoBVAEAP/iw4QcRpd5zfXOloWZiktU4AX6l21U8JEKaalPCnLfQgZDkqytB6CGzUVR6WY1qTEGkvV4I5+xXXEJxt+fHHXsQubCOiwM/FUkMF1hE353PysMXzYyrh5CQkKWEgERwikIorykZ1CjPD+JgMliCFWGo+pkixQEl5DMmvpgdV3jCkEdYiKRujOILgYDPvEGEnSy5ajEG4iwrsvwmJE4+sX8V4ADA+53nUhR8lY+G0CAvLeP0FQUXoIbs38c+xdGdoWQeX/KgAAAAGRBnhNFESzfds3OQuGryvy4a37oRoIthAMOcc9CY82PplaCS994eu82tOzQNCOvuGurlZ6+y7QfNFJn6PxwU0H6QdtZsRlz+Mvf++NNTGDVs6MUQCfh0RB+HSzh+CU8AkJ05cLIAAAATQGeNGpEf30S7pxybvnEQ6ud+08jPTdMdrvERSjxizABf466eLlFOSWxrCKOhr5PAi68gLacKwkefzTHBtuMb5qFVnuhtq/yBS8x82+xAAAAhkGaNkmoQWyZTAjf+mH5+VAGbhqf+vVHXSMJpkEseML7KyFNSnKADghGAQ6ISREATOR4hYc5JE3BZs4hkly7W8JFitHkEHwg4SneoeG3uZIPjyawik7JprGESf7QPViqG0a4FtJo9nYo39TuM9VxL5p6IODR3cv52l4sShLb6zRfa5tzm/CAAAAAYUGaV0nhClJlMCN/+zS++NqUf+AZr8rYlU0eb7CGXSMFtthKdJmTwvJtevCim1K+o47xCThERy5B/LEdX7CyAI3Xz9i5CTXTR/yhz21I6p+DT/RkPnyq+7UgK+/8gsdmL/kAAABaQZp4SeEOiZTAjf/6YXxFevgGa96bwJJt02fVWSBDyG9rfTEmG5722lU/Px2SeHjIw8DIBpcItKzc/0wzhahEyFSWyFxfvvdGE3vaAcw64RBH+fpS4DrAG/91AAAAIkGamUnhDyZTAjf/+zq17N9RAs+oAn631VSeLrf9T/zCZkAAAABJQZq7SeEPJlMFETxv+yNjrPvrqAIESC0PmUDUZ9lzy2bH6FcfBjP6tDdO19Vr/2gd7fZO1hOovfeviuzIXxeFFBS72UCfhMQmZQAAABoBntpqRH94QSSal4PTCSTGV//tQZ6usR/mkgAAADRBmtxJ4Q8mUwI3//sppqlQD+2QWuOAzXUudS2J7n8Ibu4avj+JjxA2509PVrytoNkm5hMzAAAAXUGa/UnhDyZTAjf/+0x0zLjhCm5qf1YxgCWtvkRJFOPeBjk0UN0wPFYi+y4mpzXt5ITJuNqFzT8kQYlpkDerTwaa/NYA8iEmQ/PImiSyGxLiUX/72QWRAAo+k1WIaQAAAFZBmx5J4Q8mUwI3//poG/x6T8oAuZbnpCUdqpXNmfzW12q1vJgeYJl0nLDUAOrQ1DFg99+R+uNMWNyO5nO0NncR0i/Fb7dj5TqLeH+XhvZBZnkpJhza7gAAAFhBmyBJ4Q8mUwURPG/7THTOY+fmeKir3J+1c4luoAy4k+FXQKB6SatxlqGv9OpiccoKvkTlM4T2p1QrC2S9BOpQtcUyf+N6qcFEGH65q4XZK+5XFC8VaynzAAAAMAGfX2pEf3eK10DEtwpCgAAQ6fQhPvHk2KckU6/7NA4XbLQDh6S6ygz2AmaS/7c2QQAAAGRBm0FJ4Q8mUwI3//pnyc7gogT91Va2uLmsqRU93EuED3d1+RwMG/JQw3LKqld6H3o/XjVbuoszHvFr/1yH6ry5MiHBsQFN1bsV1frDxCWTCJY7pqW0dyh7tEK1Snyd5htHtBZQAAAAR0GbY0nhDyZTBRE8b/pppJB1qlUaPTwAgrVgjScPDVtfcBH+opiEPtoQdLRRHh8RiqI7dNDOa9YX/On+PICSov6/badrWqkLAAAANQGfgmpEf3u6U7pBvw3yUmi1YCSSveDIuBt1JiN1wnlvzC/SHfw6ptyI7cVyWfyNC/k8hu4+AAAAW0GbhEnhDyZTAjf/+mGFQ00OEYBQDSLcIJJcLToSdr2koRpK2u5mecjcBZNra9MNzMkmaok6vFc1Ez9JbF/9oth0bFiMd1Cv/C4fKPmVKTmS0tbCaw73S5/QitEAAADdQZuoSeEPJlMCN//6YpeoC+YA/gA1UcyQcvoJn25ydWVfRt6yvZECJ/iVhWRtraN+koyqgFi8+bviq9/NUJpNqOmql+MG3L6XaZtVaUWd2486OJebVikbg4ftAhLR/v+YQEYudHMPqxdGzfgBKOr8x9AvJpqQAwFZhVaQ0n0GNR79ua9kcHZgaqOq+oFzP2QEonHWr2alCiJegEeX3+znOImnkZyfX7zRDK8U5Siek+40D8lgQfhIEzkheQlC23j34a3UiHe4Gb81L+Bxo9+D7dQYm6C2NgzXfftfE6EAAABiQZ/GRRE8n0ZnQZ3D3DpkmfUMwElnP7zXCCFqCn+dztTrkOB7ZSgM2nsMGduBeS2JPSad3WKzdR3xOnSohrjj/NHYXnFw/PSZPIhQ5BOV26soVc+RsqjhHIjLEFDjVf4vqn8AAAArAZ/ldER/UlCIIZLUJY+nbBL1MqdlT3JoE4INzGPoENv1ACTTz3FLJ5Z5mwAAAFYBn+dqRH9S1IVBH5IxgIHRZT3LrXUXqn9VZ/k//9//9W6C7QeTbMyZCBZ8n9USeTl0WWq9qf3iAjSAxSbVWPjgihKk2INc3pDWa1tFbSmF/5LH3HHZEAAAAL9Bm+pJqEFomUwU8b/6YmVrL4hgCru9f0xwr+kKXofvGWFvd5m8ujg0hxrKx7rDGXPBlJP5TMsl2UtD83r3yiIFjRFDxOn4wBlODb6qEGN+eCjYcfX9HzJHeNQpMATTRgGZ2Q7l52bRXX2uIAnt0648/P+EXDJYi+KcCOJ3clBMulUdcwUe35hE0KEVtPDkzxAm6EAlob7resBMq6V8FpDaPsHdrn19QWn6vLIzOF/5PjRu0ZHOgCtIr0KKFqxOgAAAAGsBnglqRH9SXNKJ80jFExKAvSctLDv/HOnzJPgltRtqAw2djRurY8+TeQH67LbZ94CACbhjDfYoJL2Qws7SNPQTiWpGizCW3gK2z20j49guh3JKAJmN0K/TGlsANSVENNH5kkKuD/I9k00hIQAAAMtBmgxJ4QpSZTBSxv/6Yho7ZHwAcUffJbhqdF2pM9exfBXmqjSEO/0WT5P0IYZTHEWYxTCRkoZe8IrOeo/P5wfTzQjt7TF3B3hG9Ry/BmqEgnkp1XY7esXV9iyzx0XcTps4gasD4L+VN12+ZpJcZSmZYHFgnuLK0u5n3mJm3BteEHsxLh4eM6jaen0H3KkhKutazHolw5ZzW7sZWEpVwRgIvXJ3ZaZyV56PiwAc/BbHsUThSdTOyeVnaUinbqzioLEBR7nGUY4vKVJY1AAAAD0BnitqRH9TMc5RXvlMEUAXMqXCzoTjd26fqMCCgzE/EgqbHmBYSwWRn99rpqhIvHNe7vv8JAUiuGd/1mekAAAA6EGaL0nhDomUwI3/+mHMI5HwAyWiVZyDFVs4JTm+Wvs8NnW+kTYvQsK3QiUtA1ppb+NU4E0exJYdfD4DCZQYLIMwl2ATEm7XpeihaLsAgJ6b6WKHwCm6L0UYXAp9Vhb3Gl4dI+xWU5rYMXx9S4hAWiWsseyiYiV2/PME5mFzP1fgxnyk89z8O0xZXVBeTYvqPLRWL8Ws2YulhMwk1isWtlqSUYIfG630dzCa1Je2JOTzbHPbLRTP/TFSIHc96CILTTWqK91yLreROr/SnzRzKAwj2clBul64grbucgMvD7egYvQkE0hkJ0EAAABhQZ5NRRU830s5NnlCfeV1v7d6ldjC+sPhgL36C1YGFfMT96w6ITzccN1Qj6S+twHfQM7Q+jL19JgGGHjTYaq2KpT5uaGleWWo53EfC/25tbT99wobrr7mBveNbw7ynRg+NwAAAG0Bnm5qRH99Et2F1ZqrRHj/reEH+ivz4EQWfRtHO/Cn6RyHHoHyv0/JiCeqQ9+pUFSFGugk/klEtMxGHQxOp9yaKmfcGmAyQYJHPNNn03CeMg8yFVrKCQuNlqEkL4fScTLa/eN7atjG3Ko9BU2BAAAAvEGacEmoQWiZTAjf+mHXTZ+AGaTcv7wbsUiJ6qc9N+RHPRjeD6RA4rrLproYbO1dMqw8yzcjLmTVRv7LojKjjg/4u9NfBbbPxF+yxm52nXcmbL7tzdpPD/kSC2f0PKAXBpmjwNhY8Xl9CF/1Ijf37t9LU91QqksPwedO4vlFUqL/tSbGwTkAeUIwvuNrXtvtzPq5Us60cqcNOLlz3sLeWGx+Arorg4YtiBbSwO8paG6AiuctXDYQHKanI4VtAAAAvEGakUnhClJlMCN/+mHR/erUE9usVAU4KluXpN5NG2yDpcFsjk525+WpF0k4a3pHdPlmBC65mKLjZn0whdE34QPSzY0DLhwC1igDr/27hVJyIu78pw+HhPb03k6mN/SkdpvxtRVYpMqid9wf721s+5fHOcihtFNmAhK4lddhjHJXe8sLmD5r0nFdL/BwyNDUwQ2EhxnDBZaqoeYrhnxvlUBaCY3/Z3eKw/3wiPVNyiVGIZwvdMPaF3g+uInQAAAAzUGasknhDomUwI3/+mNRwxJTCTXq7TnpTJ7GRhW1L1XJzYZLH/nUjXHXiXfdjUQAPriKmLonAu494u9soD4nxRIZEu2FjG9J0IcqiUXn2187cROm1d1e5+N0RbqjPOrNs0tVFF6Iu1US1pJqcI6oKu7pAnOZFKugfbhJmc3ttfquC6hWF7ulZGR0pPDBMWKBHXoLsK09nhlY0RoyPtnvB4TorA/wUw10IijZLrDnQyCbsvGG036C2XP+r8sT07AZzkHIZnhYD3puFAXAo8EAAAE2QZrWSeEPJlMCN//6X4CQY3jhQDHUyWEzg9ABel+mvhEmbY10EbXyyHSKvPO1+47WknFwP5GtYXUqqydh+36dpB7HUwellSHEsscEvqVigSTSvjchRgtIuBGKrlxBbNlAdD6MkHsHNCZIvG0ZcS3nqGwHRXxFUZuWA+39+O8MSFH8LS6QJu7J/eFbetjEqz0MxCjcO/faIdo0iXycj9vA0SpEUq9dhcZ9aROgpDLYXGwU93yRajqHcNOCkPJ6f9DiW2MASxvT7QWIFaHi9p7mN+owAo5qQMR/vrWE4Rxb1cbXC4Mg39ByytnHKjsmBAZ6b4mENRQSVvBD07Sp0QHAluoSyjgNdU7L6yJh//dDe1kq+6eEnqVcrs0aOmljOMBK852xS7rmbYf/sDcmNXzfzSNpzFZlQAAAAK5BnvRFETyfRsZnR1WOetYJyLaoiTlQ08u7mCGxg1dl2c1UmzC0qah+GDgD5Y735ur5Ag6F2JVAbCxtHBh1E0d1CcUSoB79AGMSn1o1f9MSJjOhY+iRRfANd6m1/7uJnCwCZ/hmXIKMSp4EmAkarUW0+iOrbCkgqFTmgLuMkWaehfp+djKX3qtXLqTLhAbyXFef+zEI6aPmYIB98FTkTfNIomy3EFcIS/Vr6dUFysgAAABoAZ8TdER/UolZDirZeOA/fs/JLxxUPKvvUwDNZ2Df1MlcmiSzSIxkBfJbb+WPQ5yB+0huV3Qicq5s1czbA+7ud4fuWIwowto3Vc8Ouh1O/aYLpV4h+49zxH2PvR1SnV61jQGsjWJ2v0EAAABZAZ8VakR/UolSOHO2CIExk8aBjDRh+z/xJufbMh2ef9Uq4q1RCQQfsPSL3FTau5JkpcGTA21kYR9rKzeTKVcC5VXD0+DaQ0pUWHWdudpUK5k06+vsvuh+3X8AAABxQZsXSahBaJlMCN/6WUgfwBLHSyPmwpg6fS/80p/R3KqdrsIqgVsUg850iqdeubMyWU9DRFkSupfqN4FSFHMvu5IucAHVEchhMNzerpNB4aFYNPTkSkaedS0krIjxHEKyteCfjCt2Z/1dT/+P0Xr2n8EAAAD8QZs5SeEKUmUwURLG//pZD27URHDWtUiaAyVPrdMwXNagWxtTGSui8LqH2eLTU52Pe5XRnAP7vgWvqQUsRJ7NkFThE3GZ2fllcvu0CI3kQp4NIYpbn+XXemcoAeRiGTjATo5V0Ga9JlJDKLz/DC4KKYOLzecsjQFd/POyF2fn/9Fd5Oq1kXdP5ds4uujDreu8vLhGjt1AYP+aZ4nQa1alm6+erqjWMmuaJ3q3DD1xAh1tEfAoZ6F8X8MgfvBCVNKfza11iu9x04DjeUSOOrDBOgvks/WAl7mGlXIozRs0TENf4dcitPluDi5/JPf1m0xpYFHKS4JQrOp9IM8fAAAAOwGfWGpEfzmat9GJe6qv7OtBc3Wo8c6nA2piMs+k2nAZq8XNJsbu6EoVXpl+Ax22sbpCjjNk2qMnmODYAAAAiEGbWknhDomUwI3/+l75Uy+QCNssU/j0Z6o/mg17kcoiWCGZwYtDNbrYysiJc9IPFsGdubV6d4Rd6hZPAbwGrU2YlWBNUVDri9wgaERCwLehYhOjnAxiTVzgkPUUpFcw2heXu11DiIZNYp3/5edbXbdW72x6Rxs+CY4UlupMtZPMbFEYEcRi6EEAAACYQZt8SeEPJlMFFTxv+lj4nwEJYrFVBXkDf38kO+oadjhtrwvDV0k/7OOZI3u5QHzBq5d8g54KbiG40ll6QQeCcQD2IGEr3QVZpATQKvTHr9dny2KBtNtFMsf/6/lDvZz82fHEW9GxchINtzHt0Wwyvr++yJw8yRt6O3omCgWWtJu6m4QlHWix1QIBpwIdMuZBPOj/xBn5eFAAAAA/AZ+bakR/N9o+Tmnk5ijjhebrrhPwjLhfwwIen0ZAvyUjyNZ/smSrjKz2y0Jr/HH0wFCMsXb4ZKHcc3vdhoE5AAAAbUGbnUnhDyZTAjf/+lj4nwESz+EOQd27vxCvV05tSAO+AjYIpMmX3/LuHqNM6bcyheUpV1kfBeakn9Qz0exqOUitkR2Zb+hVyJd06DZmvdJJXlp4IbDn5PRzNbGhD77Qd4BEudUV5Tq1IvMDwoEAAABuQZu/SeEPJlMFETxv+lhAKvOOKqgC+q+NLIpMeggGrX5dQFzS47N9ov5ubl2m3jZP4KImjVoHxpSpRLKBgmxXv4XUq5jTPvp73dNbFWGNNlRxckc/a9DjTNhkEBJ7AwFkzuoYtXtgP/3ljEfJPagAAAApAZ/eakR/LdYoLjNWxquGJjAOAMDYcoErgivecjlAr+sIjDd/O4DWlFAAAABtQZvBSeEPJlMFPG/6WDygIwpao1EhYUAOGCb105qpqpyMFr1ihBiAMX6G8FmVNgypFX1W5cbxjqvsmmgVcLZnjSF/sG3ZvpJEWr+7G0kKx6m70/1FiAWPsyEtamnSs6f5d2hx6DcNBD4M73n+CwAAAB0Bn+BqRH8t+8ChvoxL3jmKpZKMlP68vWUJij0HQAAAAG5Bm+VJ4Q8mUwI3//pYSt+4AcoqBLMpI8HZio9Y//Oq5Zg7WBTAGD5TsTWr7NWaYEFFbf/ybZyROEf3U44GSmGSel06i3HAcUEbFGikl2AxJa/11sgLruk9/tJv7SRnpDqLNTdFB7LQsCfzmNbYGwAAAEBBngNFETyfJczeE60tH/f9KOHh27xCXhHRo+0XwYVlP9wLu860YwK7Gu4YvNc7pEg+JHtKFvF/c14TdOgomnCcAAAAJQGeInREfy+cDBOai0r33/KvvaeLkrutsv1JIc7osRh23Zm0T+EAAAAzAZ4kakR/L1RCUDP/YIYKzGKDU+vXrp/kbe756Dy2WRyRkQIOy+qQvb2Djy33UKCb2ATZAAAAVUGaKEmoQWiZTAjf+li/DCjP4WsbgByeUhQWiriybtqZUNt9YdHtdGXFxRgCEbFvQdidcGL68Lg3BLMX7CCO9jB9TRikBFmG/e4wmaht5mK8KmBMj0UAAABNQZ5GRREs3yp6YTr7xSopCKVe2/Z3GHYzIXcy6frRgkI3dIOEJmG4DVMng44Qtc/u3awBa+dGtHdbMLD6By07FOl4V0neqFam0Mf3LBEAAAA3AZ5nakR/L3CCAft6CCQdNblcOKtSOe8dscsw27Dbxb6nz+fcF197LQGGeK5yRenvk1I1VZJJIAAAAMNBmmxJqEFsmUwI3/ph9XwfwhyiEHTP7HwNixzH/E2wZuN7vNhsAg83d55/CPO02J6wWdJMuRP+UcJRYglzkBpAC1Kf2xTwRKfdsCeZHNK7fEXW3/ApKe43FDmQF0Bfn6YgkMeabdeeyNIuvhWZLwOCOVwalsJb3Om1x4pBVSOHo/Pc8fYWj453giINB0yiyf/4vSxkHKy/UOxknKg0EgmYtmRmQ7gaWiUMRkDwgvffnZhNTa45QgN7vjQxn68EqWLgoH4AAABEQZ6KRRUsn0bGf6rm30Fvvmoa2a8/UzcLbqEKYyg5nxEQFE/CO51yCkRdE0+KNCB+v60tu+1gfTwnw60XSEU+/CbjR4kAAAA5AZ6pdER/Uol4pbfCCgP4sEJJreJY5Q9R8b7pM6aPqO0giZE5hO53p230v0KjX0K1Q1RJWP9Yx83tAAAAWAGeq2pEf1KJeEVcZhgt6GYw+Ej8vqmV0V/wwSxdTwm9RGuKKWQHbNIvwDGEisNM5NPJlGZ7/Gdpnn88k9+K4M1baJZ/e9ItYzFgaKXkOU/3YGvFAM+dv8AAAAB/QZquSahBbJlMFExv+mHXPKzQEc5/d7aDYIUYhH4n6rwJ+1wRvetDKB2Gl6E/AUwJUsGjPS0x6Lsf0d06eDYjiQlqtiUwqlsN3rNTVYzuoG63Xc+1yGy/ldZAqgF0buLOx/y26RTC3XowCkc/1zkBVRi/DtWHYYOsQ9Am/ExJKQAAAFkBns1qRH9TAelGiM9HJmw5Dkl5JDDuetYU6K8cnEPUvryngaz1oagnQoGAROAc6AzXIosAu55Dy80qRLh835f2SKmpfNVweLFCI7B7BZ8kOgJLINx0IoBVxwAAAQJBmtJJ4QpSZTAjf/pil8e7HIXYYAeT1O8zOesV7H5b/NwVdxZ7J5liV58o9B9qd3hW5kDtUVF4hfMvusGxg5J10edy0b5Ba2e8R/JCGk1zwbw0J4+XsfNEWls22B7Fi27z//JVWTUWrPFlTSRdBn8/mzrlTfnyzoZnD2bVPDCfM1jomLUEzg5BR5XAjo65Wue339B3X4vs6KojroXIEZy0gyx9WvhK4n5FIy77ObgFVXNBFEf5nnXyjhttxCaeFi9wJozwL5eUivLJBHF7bGAwLzPTD3+8RIv1FqPX85IJXwQrhElaWziIkV38pQMglFrtQfJhqmbv+ejapwwrpacPZ80AAACXQZ7wRTRMn0du07mTwp1TIBYjif8TSNA4ZXLmOtUseGzJC2jjDesC5BIDJGeuOoP64OSwcliP4ITT9ZkC0l3F2xEPk8GTYhAWaHDddNKpcjwQ8jTHIPgYkGpUcGWxBaftcAeQAFb9CozPnxlqr8eFBQZDDuO5iwp9h1cf/fNhyPZtd2phnLxDyIBKAVUtFgR1nhSBdkUKowAAADEBnw90RH9TAcxMVG61R4vpEHTZeJBk/h1VVBUl1/SKVcHSTC80qkXYWHJgRo4dFvVQAAAAYgGfEWpEf1F68r4rMMccVIQBob3RKQfwLBkx8wEluodLZpJGQHsLaMnSI0xKtd3NmvHPShcbpOs6XL7x0xGj+US2Hjr099HTd9Bd/n0R34N3trpI7wjJ1zRIkyQSSWvsvRYxAAAAzUGbFUmoQWiZTAjf+mHKyfV/gGrqgtWoCcxyyhqDVOlfJiEnoDlGdx4aq5HlrgI5lU0/t4Pa5OG6TuY5r8A2cNfqZ/ASJap0iRtVIvHONt/cfOSz3Tsh+cbl8TtKV2Y4AKjsCQhI+LMgo6TK1J/nV1xPR7Za6SLAApCDjE4BRRHtru52zs9RhwyRO9TrqrkVin5NKwKds/Oef6jR2Znyh91YddUK/W81daIenu8SAo7dNeJ9eaGr4xyLj7q7ewNgFcABmvMEZEJg5Uiw/mAAAAB0QZ8zRREs30007NqvpyS2p/IV1Gmk2uXsZm/H440B87iCoJZN5uUx4HlSmBOd/mtwBqm8xibpYCPhibf0OqxctZv3n3qMPkjGFZlMVqa8KqFyujcYQ2aYuIGutfkOBDW5cOnizkuI8kd5k+Umot3pNXeBx9gAAABKAZ9UakR/UktUgfLLXD2jKuzYOkiIUQx5/sQepSGAOmDe9K+9XL8YxBoSFYXqXIYltJ2dKVmJlV65hbLeclqnNQz4b1IJvJhVmMEAAADCQZtXSahBbJlMFExv+mMIVdUjdSqgFds5SCvOod+my7OVO320ERADDtoK/prNW3EnxNo0srQThDRomEbdd+pVxIVUPrl+frYgl/EsMWFZ6TyjNO0HSYDP+Cwu37419kuv7iNUVtWP1pGUtfXFf4ahF9M9MXYqzriLybHVi//X0cknm4QN0F+Bmoy3Shoy/chBgprdte/x8jIZFk9evrcgpa1aAZjcGcterV6ZZFrYudm57K7++Ngcs2Bv6/64k0jRBFYAAABNAZ92akR/ehfYox5ZoIfESH/Nnv6q7YvVxgxEm1OkZjuUnwgeTS13GTwzBJ8KY9n1VgBeNccUl0PJLSgwgV7E0xMJ5kMECwN3EWykOwkAAADIQZt4SeEKUmUwI3/6YYJAp90RCtUAozVyTY8lUcQEi4AigrPvrOdPoE0xYDWbutCwf7nsAcD2DTgmgeIBEgBC90fZecqcwb57Gz6Jn4XEWcJtRpo3b6zvK9I3SAE+LZxo+U3qNthK71UvyMNNVgfC72QBbvCAOcTdlF14/Y3T57cY4nTtpm+WynaYyKFXxkpuJH2O5Hl0FugNbAAoTFXVQrFxAJJWQ9g+xceKV8jTX+16i0w0KaKGNpneXdfb01thc28on5mhTvEAAAClQZuZSeEOiZTAjf/6YYp2vKMZ0tVAK+BNtCsZdRIS1+AW3gP91s+/6TRzRZ7OcJgeXMHsAE4dctWjaIPNEbMV24YV7b3aXi89WsZ9/yACq/Tj78ocFehMXCtETto3mXH9Knsn/A4yXgXftfsvJGn4JeQ4ZfrfOB/NZPNOgH70hRvrfR9I8xTKiLEGOmp/qfybm3nj34D0m/ktAiTv5kJdkKIF/arQAAAAqUGbuknhDyZTAjf/+mmk+yW90z5VALGYz+GCkm+PpoJHsc0f58rfNCoYE4t+gAu0sNDgFpzRMG1/kbjyW+hrhHFFDa6CEvHE1XT2TDW70D/fvfOJQwBAYWSFBsIRu6rucqbCdWhgBZ72l0Y7EXaLvDc/4FPHO9F32fAEK+iC9D784YykAaqMumNcrKNPv65DfSUQ6URXEdSbQSfYFqp1gpXMtDB8bNU0rLEAAACPQZvbSeEPJlMCN//6yCRh4C4rSSArSUSp4OUqAgt93y4/sFfmWD7s3OiYAyH9/ZEEi0wrS0bVyog3EcC2/9lJRp8mUYzMQHBV/vBa7NIEYlzgQbMSGngT5agFkmjm5GFkTCzc4RmEakYsT1r2CNhnEN3lCXAjfwrZJolxmnHoQSjJRup7a3bqx8MfH2zpesAAAAB4QZv8SeEPJlMCN//6yGyRHnj6OzmgFAYOc7HRu7zQaXYrHp8LTLEv2VRuf1J7rxgd+nStj/w2L1JfXkTcT24i9kWNmc4UQgaHLAtIBxzEK3J9jAOBTJbKFMvjgo+0cOs5IOV0boh/sg2MX3OjeseiA/s59s6TZHXrAAAA8UGaHknhDyZTBRE8b/tMdI4L3kZxwVyvH6R9A6Ind4g60z3vRx3FkIFKNuN45Ak6tN1kN6v3Kc5GyqIZ7pwmjCC8oEeAq26sWLFGceVQzO29kgFMyLQkyWx5tEHMzBgvlk4+ga5eZyTiZE167fHjow48hGi+vEuLC6hTNDk4J1ELK78Z0IJ3TfcUtLQvAhIdD7PYBTYEbSppeTro0/7I8hLV4KXYTd393zB8XJ7kXhO7WKxZ3CFoTXmdSrUPAHul70SRpuS43BmTZfXLS/CBl6RxLb0LKYOvAktFAYYBg/ZK60x25mNxA78dpGEed8atB+EAAABSAZ49akR/fRLXSg+d02syref5P+qvW0KSfEwfD3YU+NNTe/IumT+mO90PwAdSZ1kvk4PBrAWuv8My4IkE/n+t9cvcwFRWmhdtdjcyvzgyhZCLIAAAAKFBmj9J4Q8mUwI3//tMdFo0fhxC7jwTvpDgIJwDV5++tDBhDsMGlPAebgkP4io/U39pz9s7NQ1SElFvH5bnBlCYqqqq1/Y/U/e8KFiFNV63M8rbJ2X2Djba0LoeuuTCHsbBuVy9dpFAl2zt7yQtQjZnUGV3n/wMBu+92PdBlhqgJVF+OTy5ystYElT1tNxnBxxc8jQocWO9lAqRd3npfqHHGQAAAMxBmkFJ4Q8mUwURPG/61PKg21UBhVdaNYGNrC/73ct1eHrfCUpcdB9tfH6b8xuWXqWR3FHAqsvuRSph0iHus4meeKnLpMKT86lEYC13366e+Mldg96mDNxXQViM+UTl7TG2Q11FFN/WQH/LZyReJX8cAuYg6xzvBXHoGbFPESlPfZJif5JCYNJQ+23g/v2gKvUz1psquidPeb44JZdQZByGCEVsTGMdbWgWg422C/AI2dc498nGSxp59tt5sh9srLzCAWEDT56dXDrHJ4EAAAAyAZ5gakR/b7UiOT5QaOBF73HLTMgOAh2Fu4kt+G9gvDnteOIoC4r5m0QHwLCUd8MlVUAAAACnQZpiSeEPJlMCN//7THRaNIPw99gagmfQPyxE4BNHogndJzxxZpBA+sSdY7+Qq6KtAaa+fb11NCoDpK+uLznSyWuAbaj6/eH6SI4ERpLXvHselqflohhbGrpqArfwLltTSYgBV6v2YyqkQW/wzP+XvuGhY8H/atjyAhl3ENfrVNmm/9O1+lCS4b+c4Npj8G4zzkkz7BFzDm7QkDd8u+hmQdKgznYKGcEAAACxQZqESeEPJlMFETxv+0x0V2TEwH4DgaiseBZ+KvZ+L1h6FwxrFXtMqADekv6VHo0T+LrZ5Pt48O1u3ag/O7lFNxK+TjaY/s9dCQP+dYOx54A7JNSACZR6j00lsH9I+Lzeu1ds2IBlSeM3FOGCjuL+3veDDD8feRgYP2tc7Pvwyl6FqV7fOOx8jLcnkVqGl19QAfqRJ65MmyiCuZd6zpERk2doHuWNGEZXQ3bDAWrYzt/IAAAAJAGeo2pEf3qZhuuTUnokKzQhw53Nx8fyACxC09OL/8OA05g/aQAAAQFBmqhJ4Q8mUwI3//sjjd85HodAlU1APXDS3qMPb/+GPPk8zRWBLBsdUhREgwJrsdaGdsWFnbbF9TGlV0aOBT+DEpNelrVZXR/wCIVXXu9UijgLDldZgtPuZSiiyxyMyRxd2Xz2BcoCJEsPp3/D+QsVPthCEtxIIUQ04XM1IdklTg2BgJw7b3lW7TMuHe6f9n+z/XzS3TMgpV5iASrajQzYZ/pazVjVRTI7gRJhFrBBD1xuMziEb6RCo/68apsQa9lckr80n5LONXH6VfZ0k2Rwu7NlOHJDdMR3i/VyIn3hd6nkdL3ij+yGqB19cyMEUcRJEjSUKUWgZXzEv5P7ghZ6sQAAAIdBnsZFETyfceS0pYuPs6POv3v47c2RrE3uJhcoRsXb+MELcxtDMcmrwv1ontHvhqJ6kc/RErU0qQI1FZ0Tr6lOCI4zNk26Dngm9fCxqVlFd6JSVT0Vw0/bgp/Cblk/AiHmQ33un7QctQjIGnkouVUnaTKHLLSIdWS1yN8j2dVvC6AhHQqHUTkAAAAwAZ7ldER/fRKp8lbsbCtx6GABGwoyXYPE8P2m3gDrbUm+Ji9R5BwDYgmMN13KYRbtAAAAPQGe52pEf3rzBpiU3qo9cfHQHQWUMZu+QwELHM4TVRW9lBsSH/tAGKf/jz/8Dhh0cKAEpIK7Lv3jwvFVjbIAAACaQZrrSahBaJlMCN/7I2rCLOeVrxIFHfWAhSpX8Lir//7W8AIa8TtlDLeIcho3I1o5RfY3+VEiLmoRUEOnPvplMiRnx7aU1GrrRtClOR13ovvNiV40KfOqDtL6nduT50drKU/lrZ9kW5nsbTnPHHgkGvwEZj1ws3X2vHLNRbStAnlW/1TVsHDr9/TaGv9aq2Rgpq9GXTaPXHXZ8AAAAF1BnwlFESzfcAOnlfNcSH8wbwRzV9akhdk2a3r1tr4gT9hQ0GWV/X+3zmhypi3aSkFuvtn2vjMThJf/H4FO33T7mCsZmSAqQJecY4K/SRqxLQqZ46Z75zGor/l1BH0AAAA4AZ8qakR/eHD1nLmifjvODAYkD3rB0obboi1X4Ti+oTNcPNUSugeAeCyvwwEkSmNhNMbJvEwyz+YAAADOQZsvSahBbJlMCN/7I2rgPRAwFQr8IHsNOToPAJuRdS+pQ8/yxFFNxe/e3PHKcmqRzzPMj+SjbBDah54bhfDvHnIP63b/mYrPJ7jjwiUsk092TdejDeF3lPj+jLoND9/OAOf81eHs4kk/ULMikpa1PYhD2wUQfZOZwIUPkyt4IDUyzzYvG4m130J43MUec2OA6UeoCOGcaaezmEVDVTbq6nAAgFIoDkhtzufpVKg1P0DOVEEQvM5WB06Mx+9cqu3UcDrbQeUiRk27v3wSHqwAAABLQZ9NRRUsn26Rdr2a0IUd3mSi3yeM56W3EflAuH60ubXw1i8J+AO/bfhKvzsy3KqN7QCpDkPlCbg21/akznV8IHwQ32u0mWY+Oy8hAAAAYQGfbHREf3t5cwhaZKZX2fHcACESsM5fXERyjkXzVmiCgPLUN1rua3o03aUcpNPEUM+OXLCwhCqtP5DnQvz6MSZVJgtyiCmsnjrcfviyq2QKPhM3cfRUj3f+Vuo66dXo4oEAAAAvAZ9uakR/dqCHvN+wKFJOvXhkM9hIFOc904AwF1xd8sak8dPuoALMsHv2/ZACSa0AAADCQZtzSahBbJlMCN/7THRXggfAQhYjsffB735DuE3aJJlTQNih5vLUa/HfArLCFroPykdZjvtov+TomaUKfq+Bbgyb0WMys6rMdOZB+sj2vcPvUzxSJF269yXLBY8MQtfrnRouH7pUZQhwXt62qgN3oQNiCm+0i7gN5nUsVSaMxTel646fd6iwAE/9ZhjnfnVyjGorHeToq/DdM1WNoZ8ghlfYqahTAX6fXIQ0k1M6NjXwfvWsJ1LqShlVeBIcX50vb+QAAABQQZ+RRRUsn26lM61EKO8qz+G7DdSzPtjIh4+B1QNC5Vt+aDvK93lOZt+TtEAPMZ9T8q9G108GAEFpbiXQWqjhrgI181H5VbKKM42Wa6BnBEAAAAApAZ+wdER/dpcl1WHCjymO0jpkId9iDpKdXOYIV6v+0ehX6t3V8QdA9DEAAAAyAZ+yakR/fRK/3TgcFkpRVXPRPpHSQLhsWGiwRRab844H2f+xUWzyuB0iZd/MqwTCK2AAAADGQZu0SahBbJlMCN/61hRAgFa+BgEITa4QjnEUbar/JqLWMK1YJRfLMflGAN6HRvpq/v2+Bfzdcf1nj1u+4Wrw+mo/GTkMce/9a4fu3Y2feWlEoxDsp2MKUrDVjx2OwRjquYGM0VCavHQNSXAhk2CsXQddAzBUUt+9i4i+P+U2Q8fNesyc6FR+2c974fSBNnF7HCw0L5agb5WdkaB61Y+ltaLoggxO5io2InqJisv0EaH9haCqkydoWE8sl+6B7ZJCqk4nuBXgAAAA0UGb1knhClJlMFFSxv/6yCRtugCFJRdC0R5WXrgxmhF3WC+aHT79C7kM8kpGdGPHR5HrqdwSGoSPyH2sNSgewYG6RaD79C9xzytRnZqv3FIae/kqL9/g9hpjwbvQ/7GqA1f1iCXj3lnjNr9BU5n+ShqMC+vdAPxAygCjJv+FHJJDK/zak8QN/mRvcCFLLM4DXudiXMTzgngDfDXJ/pl/bK7CnLx/i/4L+gEGaXkwuJY6E122hrDl1ykOUKiTXIjzW82rhhOQKyamq/RsFm38J4VBAAAANwGf9WpEf3TaiaH/k7mENnekBDf4+obweGtf/1Yv5G03xrWZDi+zUM6SzEvLijS5foZCLQgGGSsAAAD2QZv6SeEOiZTAjf/6zd1IzAC/qljRra/q7j7Qs03DxSWFv/ln5BntQqT77+LVwkmeAbeG76y8H5P2iERyGbi7DmPKeXuI8LP0licmAWNwmYQmAcpBqWazhKZcdp/SQy2yjnfTCYYD9zaGKkZWno9XiH05Lu+TLVbp+DMKdezbDwkndq1tq4qHNYUOZ9wBqDwd+F5Mdk1iFWQQ6nge3LI6QugACaJ5/nLwCgUZbqG7e5TfRXz0jIIsYgOHQ5alB5PGTtTCzikDq8W41DveRKlXSYy2++j48P/Y/LFZI8h7Bz/RaxToUVFuBwXXCHtAM4JonSuNwJPrAAAAQEGeGEUVPJ9m9NyFtTYhWww3AZaj8AGZYWkOMwRBZqlvGn+/9zLK0tppyT8TOF+D5cCkkC+xWv/cGeum4hImQkkAAAA/AZ43dER/bxKCUpYBnWJ8HYmN2WkmeFMvifyDxw8Ztdini1X/MWeJM20mYhNCGwtr2t23Gruwy9ve4AViGOzUAAAATAGeOWpEf3HCmQS/d9vdwqK2lixl5vYSWhb2dC6rSU17p3wZfBwByKtGDccZc5TWtwJ3sPcD1g6JgrIEkTkAdnjaY6HQyHMCOXwJN6EAAAB1QZo7SahBaJlMCN/6yB3tfQARoywdvyZpYmqDzacwDGqPOWVU+W+L+sug/ylC5vseFAN1ve5lRw+T9nvErItLDLroKfR9se5vZbHhzJLBFVuuaHuU1JX0PsdJsq8zWCRPT6rHHe3eek/rlIXnCA+rNQ3QlphIAAAAt0GaXUnhClJlMFESxv/7THS7HN+/rXziAFbJBTTrbZCTBu6k2VnMib60T9niZPKgGncWwb0BjnueLKvguYbBjSRcLP1iTKXpj6hrfjWhIZhRHtLlAmXtyPMPqy8ts80lpCt7gcdp0jYYtVV46v0/W4FdFYsQL1RfUI5H9QRpdldvnrRiV+LPjenf7ZcKX/fXmN27jt6u235o2GYBR0ut5LdY3/S6xRtB5bttlHJHMZXNHXBLCO84IQAAAEoBnnxqRH97lPMDZRwOBQ0sBh/S/G31USbVKlaTuYuPoA2/NpJALXyaLGMZ333BzfUutlk3YyytqFlgvNOfKgbclUO5giHC9coHQQAAAGlBmn5J4Q6JlMCN/+g06cApGyC1yKmAoa4reu+59EVWbgXP+6IOCuLmSXjVxptapmOhqO4r39CBq0eU+xqOc6K021O0NSXzvXLfIsLOSe35f14OaRW32eOp702JNZFDCzBReRd5NrEDvagAAACXQZqCSeEPJlMCN//oaCw4ABmpmupNwvllADODp+cGHiDNV9+jQeucmGX3AvPpQP27jcad3I3S+m6hQr8aYt12gEYCbc1HzQIXxg90vKYsf9BkgqDqeuyjEIObwvDwTrBrOYUJOadPxqkhpNuNnpkN07XYZr9oU6eyqGYInBtqV1M3a/KVriexYh/NCZSZ4YNqmZyu8nsGpAAAAEJBnqBFETyftSqiDyQx9zJdgaGyY1HATu2no6lHNIoK/0h7yci3obm3IRL+vrNh2tMWdB7KwatR0t3Q3YxpYiB3poEAAAAwAZ7fdER/u1AqhHs6Dh11J8JheQPmPvoFYV2ek0LZp8s15CJ2sJ/32MPV4kGvNy6zAAAASQGewWpEf7HZWucrSV/81KvnYV0ucS6h14IDl6qvwx1DJR0J2LrA47gN/jUrLTJOpJPqiYJqibqklYG9Zvaq2CnaLrLH1K3xW+8AAAB5QZrFSahBaJlMCN/2U8LCzQNZxyC+eqdak6eQgEWd36JN+rPkvRaiP/+xD/ld6/NFa/IpYoN0NWuNBqxzRYOBVbBQMkiY2bHvy5dMCBG5Z5UTTmZffqEQVhyKivLqxaxQyKTv/9fxU1fO3Ag7fTjv+P0XqMzRcy1vwAAAADJBnuNFESzfqBhH4vCh9iu6NApX9hgJ7qiRx3CCpN3tU/+ByUd3ACnPi7OKnE+y+OwscwAAACgBnwRqRH+1jNPP5RDdrUPLMgeESVOf+ysx1+sdnR8xR2eYBrPmtJYRAAAAh0GbB0moQWyZTBRMb/qibKJyALwBQVDOkWmHoiteCNIjxouVq1/4jET6TXO8jXjJ3BlAaa1uVSKRFK22bgM/73EqcVYd5QrbODESL+64I1ceSKcW3UxIcx4Tt3VjqRYwRu8fbt4xVeiNCg8H/v/Gjsov+JBHuM6pLKEqEoHp58Lj4RsB9CNzPwAAACkBnyZqRH9rWw47efV4YkfMB+HOsbVhikG8kMrEuQ+otRG3+2ONfCx97QAAAGpBmytJ4QpSZTAjf/qbCSmblQAL+DR5sS4+5Hyy3wb+uHfA1qqnend8PiVNPbBk9M4BRKsIeezCyyjGVWxX2HwZshykL0FGT469otG6OGT0vjdj0KvAS+5+p0jYFAI+MhU3y//+Rr4xkESAAAAAY0GfSUU0TJ9fPYbRMlWCAYTbwMkYfvx6RkLV4VQ8DVU/Y2N2MLfKty4mv+ib8droHk5n6Sl/Gm32n/lGn7lVVqtOelhjSkX//NX5ffDz4w0cVv/9xceFLxalG1L+xNwZEePXUAAAAC4Bn2h0RH9smFDb3aamoU0fMewDLgyCZGAjsEGw7/9ob51W97O88XYN5S1FK0PfAAAASAGfampEf2t0J4lgnK+isackl/j6bNiGbAOyv2xpCfW2uh5tNYiAAnUVp6gL/P11+NsXUQh9u+Iyc6IW3/yNnmkaLXraWOM3gAAAAFVBm2xJqEFomUwI3/qbhRWUJCaGu55vaqAQguCI3BwJPC/bT/EwodV6QCx2Ir3JyVjJArZPsqcxyyFQB/YRTj+esxxQCbXp2ObfquNievaBXErwmaqbAAAAsUGbjUnhClJlMCN/+swmmgVzch8UvDr6ybiftW312J8qQv6uP8JOSogBjXWCjjGfwOR7uevCjW4W9d57iomZrH1efWyB6aT49KUAiWB5EYsHKW1lyw6DEkfcV5FqD05EWD54VcIfenBp8N54mdTEwh4wpplPDFIE9zjTySFhxv7/aA3Z6IheLYHyviCXyIAfQJFyRcMdrIkrI312rDeBn7egIxv676oGkV4USiKeuADArwAAANBBm69J4Q6JlMFNExv/+tngECAwjauM+PAKKtywH3DEsMOcGToyVCJ+6jZwJv7hrzoI6zUpAOxNUEGenY//x+GwbvZyXh8eThoktu2eZYZEWGbihDFoG0eIRAmSxoJAjCeGszSXdCRnnNHKPqM8ABdzDToe28nAHQfH4hz7Gh683cawAq/Mi+usF3ZkLt3tw7EpBszqC5c87G5O+7skulrdOGb1HraDl139csxf3bHXvHr9J6kFDGyxidZDDlQ9WOTDB148jcPzt27oZJeicDwrAAAAVwGfzmpEf28vuWP+aB28YBF+PP7EAi1Y93cce+iuRkbStMuFen0EV4mUi5NhCGdAj7qAn+mZiS7T3rAUqLs7iDlVWAER2xgh1hCMxUvw02Mg319gwvI12QAAALhBm9BJ4Q8mUwI3//pfaLofgCIWogOIagX8dwMc+/cCmjSyD+WGfPjhDyOEie2enc3IZTcP34WMFzocHI7774QbJLMwtRApKx0jUR7pCQK4Qm3BoBIeauSO+ry5tp5k7DYSKTvWdnD5yY1FhuEf5LxPuLwMKegdB8lqcx+7l5nTeiuxOFi9LuNunhm0v4o+J2DKLG3ONeRtshcY6RjqHnz1OQlZ3IVco0DCv3Q6lbmA9Ix3doZ4dF5QAAAA4kGb8UnhDyZTAjf/+tsKU/9p4CSsBcdD1lZryJ+IBC5TgpxurqR83/6BzCgCMWh9O5vB7ePuZNICCbkgYfxc/QehOWwBHc3kFRl9283+T2XtbDqKWY1mTfJ0s97xk52ChTxjiewnvt1kUjLlVI6TSxATolUyfhU9Rnit+rCTBPtkfhjKjyHb7n4SS4TEhWA/A41RipvNwQpUmtIN6bsju08fAB7wjrZUdEUu56B37yip2jCq+UBO6FadqCVBt/1yOWIufNlazTODfsBhps2WXoB3dFKilzvYSOd0kmdQL4y1n0AAAADNQZoTSeEPJlMFETxv+l6bKtUruAJAr7U/76/ztW2pdL3sBIez6/43oivMBlo3maZAoG05WQZQYLTc8YtlhdbwMzK1cwg8KFceGNaUfyHY3M8K3dfSuGLKePwHhOtLq0FSvE9zljWaGqs+Ts1nn18C6JGsf1Q7ba3u8Q5dB3yglRlML6AX87G4a5EpFGwdCNygkVjoiW55GF2+av/LI1c/+Z4VfMVp95WwRqPqtTueG7T8hlE42BrIzJwkE5r1BUyDes+gd7Cslr989oQZ5QAAAEcBnjJqRH90Fr+F3VDBfc4L9HzCIDhXTpGnFkkzIeL7d5nvdJ9Kz7n7GQOZVQKeN2Kdk4wifspUmJgdU33868X0eB8oQBBlBQAAALRBmjZJ4Q8mUwI3//pe4cwChtDNuvkuW2Ou5YspeecnYSqwMsMDOZkGQHIo7msfh2WsqRr32EvqazxIst2S4hv7oMuDUvwEScwyikistERQkHJB+awjN2+JUF1JlokpDQRUp26VKapDO5eS/MO6915hwgJ6hgAhk5qM9godkZfjGC/OSHROz+9zOTxc0DNytPe1xa7WIC4tdLgBInpPp0rWiuayShFOKAz3HCVk6LthodWFh8AAAAA5QZ5URRE8321oy3rFOWISn88htMOqvV89JPQePwP3CyKfh6V6rN3iTbiKXQYa7QITDBF0TYghx7ihAAAAIQGedWpEf3QWskL4HRAUDEDjVaMYZX8QEH7qFjQpjpKPegAAAJtBmnpJqEFomUwI3/penmO5bZJCWARzvzDK9ZMRjCqnSYdbvqm2G5i8cP/F8eKdzzRg+flyx89wgASx7lEJ8JxWcKVgF3a1W3UB7LL7wjG56Yx4odL7efl+G44ZDNNnoEvhbeT63E7BLB//SSP+d07iSPuRGJApKdb8j4A9Ce++D1FpCGpzJfANiSOTwJ3LbMTY7bAtS6gAlSVxHQAAAEFBnphFESyfaDXfQI1VAhRAe1U98FH3q5z6e0KDDnxFqSBNw8h1OkSW3n7+JefAI6PhTX/f7RdgYuFt9VUn+56lQQAAACMBnrd0RH9rpD2Fi8yg6h8Zs/LcbbSUoXOF2LM2BGf4Ywo0rgAAADABnrlqRH9SiWj3mRvw0EQ3gL2sJ4AKPSzoB/TCehMcj1uYAVvZGdqlkj74TigggccAAACyQZq9SahBbJlMCN/6XzMC28dgVTKV+Lsy/WuVI8FM96wXf/zRPOQyjtfn/aejmdti+0jm2uYmZx87mYrAzxeiW5RvuNON22K+LEjBzRRWcvguUtrP529RQoz2Mr6vTwPYl++kx8ilG8HVUaD9ms54MwlGhtQWQQyJoxF1F24RT8hQq9RTFqb6tOw/0iMOTp/dF4DN9T7Gpc6ebMNQPRmuVJMlyKyaKleyqeiIy+bnz30sgAAAAD9BnttFFSzfamZxwHJpxUbKk4Eq3uX7Sx/iLJ2n3CfimLhMURQOUa/TuvSibeoTQgWG9XQJrIXqCkLhPnsfTYsAAAA3AZ78akR/cS6wYs3i3oghOcOJPfXl0tj4QWgDHv4T0nL9bTHaMsoPvrMk8Kv7p3VsGM9WbQRzPQAAAJ1Bmv5JqEFsmUwI3/rau2OsDDE9qqELK+JOBLBz+W77lI61CCb28Byt/2CoMDjF3R/f1XApdFlpSxKBP+rUBhkP4ok/f3Mfbzr29T3/97BvPJN5nIaRCOxv4LArbVxV6Wsmu0x27P1gE3wPBnTa2/J7lakS2Xlp9wXIMSNvpgcQa5FXZGHpVQndyvfbDd6jobg0epWH/psCnXwqD8CAAAAAsUGbAEnhClJlMFFSxv/6Xplq5r2WqiJLD/nDCJFbTJYJhA71fnYjxy3XvEaBq3X2e7b/2K1PH04Ab9bfF8mi2HoD90VluCv6ycoH76QAqBieJ+0iYvMePhwMwNgRVajSjIBlKRS27hdVtufdBvgMSgOuo41JMLeC9FP5TwDUk5X0V8NvmCNsjOJImYY0z+tSm+MVzq1X/aQwUFiXJVYmtKhHyK65pbAiffFlJzn6Uq1pQAAAAEABnz9qRH9y2YGdiZofdAoP9fFVAmGsy7iVueq9AQGC/rZU3OB9tHw3CY0PZzYwfzjzGiv6EXvOTweV6dp7Kj6BAAABNUGbI0nhDomUwI3/+l+s5NbuiueKUVf86YHH8UJ2kGimNy3PfSJZDQItiLFmXM3N9o9TGMdxc7xwVaFaYCxUxJ2+nMaIjLfXwH5kwulKsG1j1Gal/SpxKF/v/2xZUhJwVqPpTZs1cc0JxXq4oG7vrIGcoI67Pm/WMHPXov0XloJucdk6KUrVe+HV1j0ea+TNGJBfrohOw/u4KjD0tuaTrgQ0J/IVUDXx8U/phaBp6krBUl6V8tun08L5jrrTxeuA5ndB9lFLF0MtQugQ+bVSiFtSGox6BQ9fG7b7/d0WnmQ6pb+4Wg6xSQy6/qyuFWxZyFT8MJ0TZQi9KrI6+uaIDf55BaEGT0i6gF+N1fdD5NBM1vwEQLX4Y22Ltcd+TCWNC0Wb1K13o9VSlUPXi8qY5hCeam0FfgAAAFZBn0FFFTzfaaUbi3IMejqlPBBVp4JnGncuPlo4SBI+8wHDA74D0l4YSaoAaH3V/4wCb5EbPflAsq6tMNFrc0V/I5BJJILyyl37R9XgTWG+75VONMzqQQAAADgBn2JqRH9uBz0DtXgQa9/yUFL0qdnjItKn+7Lbcdi0VmF/64PH2nwy0Q/lfXP5d2aUTV9DqoohWwAAAJJBm2RJqEFomUwI3/pemYNzlFTALPR17DuX2MAueWR4Pdnf+42xdFPFF/+Z8eGAT7m9cpeyjQ3o/swydBFPgvJJnZNETNj/ytVSQcVxoeK4axrSgmnIAkcGSxP7WMFFa73muuf0Nl0u5K7qpKXJ74F4Yin7hVG+4/N690kmo0QNV+QICx0VVteNXbEBNdmp9ESXgQAAAH1Bm4ZJ4QpSZTBREsb/+l7e+Fvn8Icqe2+mfXTKIge6D3oZW6I2orDfLXrkXzwkyaHuUdq4G5+CT59+JkHoJsElns+xjPZqqeX0WAtZxTocIJiPBWqYctZpK0Pnf54KBFUjqXY4+E22KJDY1isllHrUclgu60rBHeRm1VK3KQAAACgBn6VqRH9K7fZ/mcmaYCESaMa8D5Yw4eWNDXHfG8sY8Dcyt2wJT/J7AAAAc0GbqUnhDomUwI3/+l0ungoCynAGIusbF+2vjR6V9e7YF/DbEx25MxgIInpBVEMlT6XsgfxIuzcmV007IubHHtWJlPdlf2/5WLk4C8s2hVViBSa9xk7J4hdjIX+A+NkMfzabPNdMCR7SvU04y2UGkbf9MIEAAAA2QZ/HRRU830PPQ0IUHljE/3o2CYUBYa0+vkdbLPIBxt+8Ly+A3UPmQl9nxZ4kuqqGAC/9EgGmAAAAMAGf6GpEf0oB8KXkrPZ/TFA7tRnkm2OiqpvtcDct3473k6JZYf+hd0vROtH7s96MgAAAAFdBm+pJqEFomUwI3/pe+Q7IpWhwE9aaVH3iAFVFrowW2ucGyoP5dhYhmKK6YQuNZrl+35mOx/r0kBukLwhUMgEaugVBpTKBuVd6URzFVnldHJp3PEyuEHEAAABvQZoLSeEKUmUwI3/6W6/7SbwWba9AFwLbKB4CzepBLxSiQzKCXlobO8ed58S3B8umTVU22VkjQrce+Du0l834RhChyf/tqcO45tabtFAg9u958YId25FC53z9nXGZLcYm00oTdDuvhg9tCYKBPYuAAAAAYEGaLEnhDomUwIv/+liy5RdpJF78gAIp+5M7DFvkHGi+qWj8OwZ+XHD7wqrPruoA9EsVKmOv2KyijiFIQZ+4h5zRuJXNPNpqjyvqrcItet+4c/TaZYjGWNhMZBQjZ9UMnwAAAE1Bmk1J4Q8mUwIv//pYxPF9SOwFQnXwWBreqiKH6nXRQXDVQM15MvuMpfq1nNG9ZFc0theIBPPjOq35R4jtXH7DOAoGR5hhQ5kbxgzx1wAAAF5Bmm9J4Q8mUwURPF/6WMVLBJfvng7hLgXOarzrDSfpeGu/kJbFcIs6uxH72f4/A4w8rKMGEQh7RRp55AUs3ARXIKZxt0Z7uUfKx0aDHhmm426how3w2LdtRds5/mT5AAAANgGejmpEfzbse8vWFEsZc2w66ZDIu7/RTqAGQyLvjcwDciRcIR1skaiXv6HWXvyzAn3njHSIUQAAAH1BmpJJ4Q8mUwIv//pYw4qQUHTLf8DI/hX/7/ipN+P8J12YMV+brfEwMO268ohjPwWK6v/PrEwF7X0jS9uh8qaOGzvOTZfXPJzRPUOCvbUHgsvFSKFU+l2YzJUcKB6GGIKoDWZKCg30rD/vaT31ueLUl0QshJpLLgvt6iqEIAAAAC9BnrBFETzfMP512a/DE2JWVoDwMFYse5FyA6DOwbgDvGFwJU6/dCwUkN/TwtrqqAAAACABntFqRH8uBqH0YaolHQ9aD9IY9IH8E+aWD7uGoj76gQAAAHFBmtNJqEFomUwIv/pYP7pt4BYuyrSYfS5bSLhx/+v1WmzGX5mW4FPGJFGebGH9sPT7oVdftpOAmpFOgIk2kY9sZ5P6A/57N8YotFc05c6/9Wvi9nBvMcM2x2oY4+Q8ZRqm/y90xI+7GJ7JcDQqHaN5uAAAADVBmvVJ4QpSZTBREsT/8yCkBd6S+eN4AJjJa556fLTjZU023CDu4/m3tUWPwH+o8/RMjjAbHgAAAD8BnxRqRH8vXzzWO2kifGQWl7WYA6CXYbqhEehZSaMvjA5b4tCiK5fQnVISyoPPKvvPpStstXSwT56wYU0T950AAABHQZsXSeEOiZTBRMT/8yCW/NqfzlBlUeOAexWoTzEMcS17+FxJSWet/SJp1TZpfKO5fz/XzG7Japm5sYqPXdLNfkZF8/zndLIAAAAwAZ82akR/LrSLBBVkqODiObEDoHbpSlCDS/ed9zAqRUCIdcV6eZHWM55t/lcFrpIRAAAAbUGbOUnhDyZTBTxP8yJ3fsJrPgOhCEDo3SUJuH9fU+A6BcoTqc6l1MjSguK8utUTr/O5sd0tnit8ZW8vCvAgx8i7Uu+7RY4HU6nKyZGUl5zDad3EzeTY9BcRjVDtJjBM9/AxwIMqMG/q7ify6qEAAABBAZ9YakR/O0PIOxCGI1Me0daFpDUshfulSpjMxElqbGdGp45sHHiPGfteCZy7F+sFHED9ZMj7L+6q8cxf6iAPhKAAAAA/QZtaSeEPJlMCJ//zIpX7ONu4SAiNmiAwCxhqbu5sPscx5gsCwqb1O4Ob+5aGvzMWOf2xs2vgLh08gqjSw6zzAAAATUGbe0nhDyZTAn/kRdXISJgYq0jsesTATRQO3XQnVfkNJIqoUSN+AJpUyHDgmVsZacot8OQSPg4k0qavr/5fRVz7vPK16NHvhBeqs8qAAAAAT0GbnUnhDyZTBRE8/+RF3RLYhqwDFDaUZ/+ePs4cejnLEl/HnxBsG1Mgm+m0btMusCTim+I5yGXwzwflcvQgfOMIZeBrMGyZ45LLjUOmkUkAAABBAZ+8akR/O+15kqTeT8RRrkzEGhkTKf0dlgTAs5yVbGh5OLE6oeHbPKGrKr3G4Pq69mIlr7vnW9dWJNvfO0MUGOEAAABOQZu/SeEPJlMFPP/kRXozMVgDu43sCRD/dwCqvQbrvvnwfKgZOeZX1qXZG7M5BcorhKLcDgJjGni2/n7z3Fx5YP4P9yNe+WSjj1+ywVZUAAAALwGf3mpEfzxjjUIOj8QfHfY5KG61f770VHZ1uMdbf0XGezFLiwAUDS779MLLb2TQAAAAUEGbwEnhDyZTAl+HDrxAfcCCkAMsyJgDe5xeXcnvLaW7/z4ZC5t9+67QaXNMirZ4tZ0q3fucAzL+sHWlqpAg2/iauwZeKKWN8Qmo8QoP9XPBAAAAOUGb4UnhDyZTAl+HDr9gFgjLqNPlpBKXLnPPFTtNTCgTCkFDjS8LAVb4MQrpXbk0BbNU1lo6HnL84AAAALhBmgVJ4Q8mUwP/AJNVdEhkVDSI5+PRXYu1Dcn20G7wmrOHHp3dv6Bg1feiyG9xAMymgREV/WxZ9SU/Y603v+zsnw4gNK5VasORORfNCRHIp0t395WFLkH7cmqnZTJwTC2sYkr7IHzALLeDPdJ7ioqsARQIkJNz4v4ELOsk5FfuRjyADYRwfFGXKbffR54zgFPs325nEQ4jw3fL50tScrbZPzC885v0JFC8xTyabxRPdoZFpTmc0ZWBAAAAJUGeI0URPJ9BLRSbKg/zNE98WESPGbOLQPa9chSdcUzbfnAi+IAAAAAbAZ5CdER/O52kQIiQq9V4FPhnhkgevX8GrHLRAAAALQGeRGpEf029oi8qW8Y/DsaGnNohQ+5pvVy1pxQz/WVjgkIR3z/GlSJyKcewgQAAAElBmkdJqEFomUwU9f8BJkwF31/IwkoTwcPuf9QPqakshigXRQTYYtuIqv3mg/J0BYGqH+bGCvz4GbQ0KCkKPbfyodrM0vURZi6TAAAALwGeZmpEf0sHYf6KCiMsBwfdpASighOV5K9FQ9ypTYAjb4A4J17BPgPoWED9bF1hAAAAjEGaaUnhClJlMFLEfwOv6kJ+hFdKa6Gfv3ynOz95+H8HHfYUL1BW/Xdonirga/ussuLrWekNcRulgUmaPBtfgcsOt/euTMPSxIfViTz4x9TTdORw57Mp8ruYSR9C5wAPYDdQ3QHdwX+NloAApDUgc8iyf6pHmnXk9U1Y6lxsn8LV3hLFbXyNGorE9ac/AAAAKwGeiGpEf0sHHs25CB3dD/+VXNuKXGDiLs74yX5dGhSMdmQhJEZn3yGI/0AAAAuAbW9vdgAAAGxtdmhkAAAAAAAAAAAAAAAAAAAD6AAAH5AAAQAAAQAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgAACqp0cmFrAAAAXHRraGQAAAADAAAAAAAAAAAAAAABAAAAAAAAH5AAAAAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAAGAAAABgAAAAAAAkZWR0cwAAABxlbHN0AAAAAAAAAAEAAB+QAAAEAAABAAAAAAoibWRpYQAAACBtZGhkAAAAAAAAAAAAAAAAAAAyAAABlABVxAAAAAAALWhkbHIAAAAAAAAAAHZpZGUAAAAAAAAAAAAAAABWaWRlb0hhbmRsZXIAAAAJzW1pbmYAAAAUdm1oZAAAAAEAAAAAAAAAAAAAACRkaW5mAAAAHGRyZWYAAAAAAAAAAQAAAAx1cmwgAAAAAQAACY1zdGJsAAAArXN0c2QAAAAAAAAAAQAAAJ1hdmMxAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAGAAYABIAAAASAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGP//AAAAM2F2Y0MB9AAK/+EAFmf0AAqRmyjG0IAAAAMAgAAAGQeJEssBAAZo6+PESET/+PgAAAAAFGJ0cnQAAAAAAABaCAAAWggAAAAYc3R0cwAAAAAAAAABAAAAygAAAgAAAAAUc3RzcwAAAAAAAAABAAAAAQAABUBjdHRzAAAAAAAAAKYAAAACAAAEAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAEAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAEAAAAAAEAAAgAAAAAAgAAAgAAAAAFAAAEAAAAAAEAAAgAAAAAAgAAAgAAAAABAAAIAAAAAAIAAAIAAAAABAAABAAAAAABAAAGAAAAAAEAAAIAAAAAAwAABAAAAAABAAAGAAAAAAEAAAIAAAAAAQAABAAAAAABAAAGAAAAAAEAAAIAAAAAAQAABAAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAQAACAAAAAACAAACAAAAAAMAAAQAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAEAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAEAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAEAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAIAAAAAAIAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAIAAAAAAIAAAIAAAAAAQAABgAAAAABAAACAAAAAAUAAAQAAAAAAQAABgAAAAABAAACAAAAAAEAAAQAAAAAAQAABgAAAAABAAACAAAAAAEAAAQAAAAAAQAABgAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACAAAAAACAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAEAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAQAAAAAAQAABgAAAAABAAACAAAAAAEAAAQAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAIAAAAAAIAAAIAAAAAAQAABgAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAgAABAAAAAABAAAGAAAAAAEAAAIAAAAAAgAABAAAAAABAAAGAAAAAAEAAAIAAAAAAQAACAAAAAACAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACAAAAAACAAACAAAAAAEAAAQAAAAAAQAABgAAAAABAAACAAAAAAEAAAgAAAAAAgAAAgAAAAABAAAEAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAIAAAAAAIAAAIAAAAABAAABAAAAAABAAAGAAAAAAEAAAIAAAAAAQAACAAAAAACAAACAAAAAAEAAAQAAAAAAQAABgAAAAABAAACAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAgAABAAAAAABAAAGAAAAAAEAAAIAAAAAAQAABgAAAAABAAACAAAAAAIAAAQAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAQAABgAAAAABAAACAAAAABxzdHNjAAAAAAAAAAEAAAABAAAAygAAAAEAAAM8c3RzegAAAAAAAAAAAAAAygAABS4AAABpAAAAZwAAAC8AAAA+AAAASQAAAC8AAABBAAAAaQAAAC0AAAAqAAAAWAAAAHEAAABMAAAAjgAAAGwAAADcAAAAagAAAGoAAADoAAAAaAAAAFEAAACKAAAAZQAAAF4AAAAmAAAATQAAAB4AAAA4AAAAYQAAAFoAAABcAAAANAAAAGgAAABLAAAAOQAAAF8AAADhAAAAZgAAAC8AAABaAAAAwwAAAG8AAADPAAAAQQAAAOwAAABlAAAAcQAAAMAAAADAAAAA0QAAAToAAACyAAAAbAAAAF0AAAB1AAABAAAAAD8AAACMAAAAnAAAAEMAAABxAAAAcgAAAC0AAABxAAAAIQAAAHIAAABEAAAAKQAAADcAAABZAAAAUQAAADsAAADHAAAASAAAAD0AAABcAAAAgwAAAF0AAAEGAAAAmwAAADUAAABmAAAA0QAAAHgAAABOAAAAxgAAAFEAAADMAAAAqQAAAK0AAACTAAAAfAAAAPUAAABWAAAApQAAANAAAAA2AAAAqwAAALUAAAAoAAABBQAAAIsAAAA0AAAAQQAAAJ4AAABhAAAAPAAAANIAAABPAAAAZQAAADMAAADGAAAAVAAAAC0AAAA2AAAAygAAANUAAAA7AAAA+gAAAEQAAABDAAAAUAAAAHkAAAC7AAAATgAAAG0AAACbAAAARgAAADQAAABNAAAAfQAAADYAAAAsAAAAiwAAAC0AAABuAAAAZwAAADIAAABMAAAAWQAAALUAAADUAAAAWwAAALwAAADmAAAA0QAAAEsAAAC4AAAAPQAAACUAAACfAAAARQAAACcAAAA0AAAAtgAAAEMAAAA7AAAAoQAAALUAAABEAAABOQAAAFoAAAA8AAAAlgAAAIEAAAAsAAAAdwAAADoAAAA0AAAAWwAAAHMAAABkAAAAUQAAAGIAAAA6AAAAgQAAADMAAAAkAAAAdQAAADkAAABDAAAASwAAADQAAABxAAAARQAAAEMAAABRAAAAUwAAAEUAAABSAAAAMwAAAFQAAAA9AAAAvAAAACkAAAAfAAAAMQAAAE0AAAAzAAAAkAAAAC8AAAAUc3RjbwAAAAAAAAABAAAAMAAAAGJ1ZHRhAAAAWm1ldGEAAAAAAAAAIWhkbHIAAAAAAAAAAG1kaXJhcHBsAAAAAAAAAAAAAAAALWlsc3QAAAAlqXRvbwAAAB1kYXRhAAAAAQAAAABMYXZmNTguNzYuMTAw\" type=\"video/mp4\">\n",
              " Your browser does not support the video tag.\n",
              " </video>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}